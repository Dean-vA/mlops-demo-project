{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e05673f9",
   "metadata": {},
   "source": [
    "# GPU Test Notebook - Verify Training Environment\n",
    "# Test GPU availability and basic training functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d7efea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç GPU Environment Test\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "print(\"üîç GPU Environment Test\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a221f0d7",
   "metadata": {},
   "source": [
    "## Test 1: Basic GPU Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29b2b325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1Ô∏è‚É£ Basic CUDA Detection\n",
      "PyTorch version: 2.7.0+cu128\n",
      "CUDA available: True\n",
      "CUDA version: 12.8\n",
      "Number of GPUs: 1\n",
      "  GPU 0: NVIDIA GeForce RTX 5090\n",
      "    Total memory: 34.2 GB\n",
      "    Multiprocessors: 170\n",
      "    CUDA capability: 12.0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n1Ô∏è‚É£ Basic CUDA Detection\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"  GPU {i}: {props.name}\")\n",
    "        print(f\"    Total memory: {props.total_memory / 1e9:.1f} GB\")\n",
    "        print(f\"    Multiprocessors: {props.multi_processor_count}\")\n",
    "        print(f\"    CUDA capability: {props.major}.{props.minor}\")\n",
    "else:\n",
    "    print(\"‚ùå CUDA not available - training will be slow!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b983a31",
   "metadata": {},
   "source": [
    "## Test 2: Memory Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8d64610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2Ô∏è‚É£ GPU Memory Test\n",
      "Initial - Allocated: 0.00 GB, Reserved: 0.00 GB\n",
      "Testing large tensor allocation...\n",
      "After allocation - Allocated: 4.00 GB, Reserved: 4.00 GB\n",
      "‚úÖ Large tensor allocation successful\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n2Ô∏è‚É£ GPU Memory Test\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    \n",
    "    # Clear cache\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Check initial memory\n",
    "    allocated = torch.cuda.memory_allocated() / 1e9\n",
    "    reserved = torch.cuda.memory_reserved() / 1e9\n",
    "    print(f\"Initial - Allocated: {allocated:.2f} GB, Reserved: {reserved:.2f} GB\")\n",
    "    \n",
    "    # Test large tensor allocation\n",
    "    try:\n",
    "        print(\"Testing large tensor allocation...\")\n",
    "        test_tensor = torch.randn(1000, 1000, 1000, device=device)  # ~4GB\n",
    "        \n",
    "        allocated = torch.cuda.memory_allocated() / 1e9\n",
    "        reserved = torch.cuda.memory_reserved() / 1e9\n",
    "        print(f\"After allocation - Allocated: {allocated:.2f} GB, Reserved: {reserved:.2f} GB\")\n",
    "        \n",
    "        # Clean up\n",
    "        del test_tensor\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"‚úÖ Large tensor allocation successful\")\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        print(f\"‚ùå Memory allocation failed: {e}\")\n",
    "        print(\"This might indicate insufficient GPU memory\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping GPU memory test (CUDA not available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e091a3",
   "metadata": {},
   "source": [
    "## Test 3: Simple Neural Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34fab9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing CPU training...\n",
      "CPU - Time: 0.32s, Avg Loss: 1.5625\n",
      "Testing GPU training...\n",
      "GPU - Time: 0.04s, Avg Loss: 1.1197\n",
      "üöÄ GPU speedup: 7.6x\n"
     ]
    }
   ],
   "source": [
    "# Create a simple neural network\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(1000, 5000)\n",
    "        self.fc2 = nn.Linear(5000, 5000)\n",
    "        self.fc3 = nn.Linear(5000, 250)\n",
    "        self.fc4 = nn.Linear(250, 25)\n",
    "        self.fc5 = nn.Linear(25, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "# Generate dummy data\n",
    "X = torch.randn(1000, 1000)\n",
    "y = torch.randn(1000, 1)\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TensorDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Initialize model\n",
    "model = SimpleNet()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Test CPU training first\n",
    "print(\"Testing CPU training...\")\n",
    "device = torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "start_time = time.time()\n",
    "model.train()\n",
    "total_loss = 0\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(dataloader):\n",
    "    if batch_idx >= 5:  # Only test 5 batches\n",
    "        break\n",
    "        \n",
    "    data, target = data.to(device), target.to(device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    total_loss += loss.item()\n",
    "\n",
    "cpu_time = time.time() - start_time\n",
    "cpu_loss = total_loss / 5\n",
    "print(f\"CPU - Time: {cpu_time:.2f}s, Avg Loss: {cpu_loss:.4f}\")\n",
    "\n",
    "# Test GPU training (if available)\n",
    "# Test GPU training (if available)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Testing GPU training...\")\n",
    "    device = torch.device(\"cuda\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Need to recreate optimizer after moving model to GPU\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        if batch_idx >= 5:  # Only test 5 batches\n",
    "            break\n",
    "            \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    gpu_time = time.time() - start_time\n",
    "    gpu_loss = total_loss / 5\n",
    "    print(f\"GPU - Time: {gpu_time:.2f}s, Avg Loss: {gpu_loss:.4f}\")\n",
    "    \n",
    "    if cpu_time > 0:\n",
    "        speedup = cpu_time / gpu_time\n",
    "        print(f\"üöÄ GPU speedup: {speedup:.1f}x\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping GPU training test (CUDA not available)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653dfb6c",
   "metadata": {},
   "source": [
    "## Test 4: Transformers Library GPU Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4f0f01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4Ô∏è‚É£ Transformers Library Test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\deanv\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\Roaming\\pypoetry\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing small transformer model on GPU...\n",
      "Model device: cuda:0\n",
      "Input device: cuda:0\n",
      "Output shape: torch.Size([1, 12, 768])\n",
      "Output device: cuda:0\n",
      "‚úÖ Transformers GPU test successful\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n4Ô∏è‚É£ Transformers Library Test\")\n",
    "\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "    import torch.nn.functional as F\n",
    "    \n",
    "    print(\"Testing small transformer model on GPU...\")\n",
    "    \n",
    "    # Use a small model for testing\n",
    "    model_name = \"distilbert-base-uncased\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    \n",
    "    # Test input\n",
    "    text = \"This is a test sentence for GPU processing.\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        model = model.to(device)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        print(f\"Model device: {next(model.parameters()).device}\")\n",
    "        print(f\"Input device: {inputs['input_ids'].device}\")\n",
    "        \n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        print(f\"Output shape: {outputs.last_hidden_state.shape}\")\n",
    "        print(f\"Output device: {outputs.last_hidden_state.device}\")\n",
    "        print(\"‚úÖ Transformers GPU test successful\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚è≠Ô∏è Skipping transformers GPU test (CUDA not available)\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"‚ùå Transformers library not installed\")\n",
    "    print(\"Install with: pip install transformers\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Transformers test failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c2a5fc",
   "metadata": {},
   "source": [
    "## Test 5: Mixed Precision Test (for training efficiency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65db48d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5Ô∏è‚É£ Mixed Precision Test\n",
      "‚úÖ Mixed precision (AMP) test successful\n",
      "This will speed up training on modern GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deanv\\AppData\\Local\\Temp\\ipykernel_39504\\54005202.py:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "C:\\Users\\deanv\\AppData\\Local\\Temp\\ipykernel_39504\\54005202.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n5Ô∏è‚É£ Mixed Precision Test\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    try:\n",
    "        from torch.cuda.amp import autocast, GradScaler\n",
    "        \n",
    "        # Create model and data\n",
    "        model = SimpleNet().cuda()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        scaler = GradScaler()\n",
    "        \n",
    "        # Test data\n",
    "        x = torch.randn(32, 1000).cuda()\n",
    "        y = torch.randn(32, 1).cuda()\n",
    "        \n",
    "        # Mixed precision forward pass\n",
    "        with autocast():\n",
    "            output = model(x)\n",
    "            loss = F.mse_loss(output, y)\n",
    "        \n",
    "        # Backward pass with scaling\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        print(\"‚úÖ Mixed precision (AMP) test successful\")\n",
    "        print(\"This will speed up training on modern GPUs\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Mixed precision test failed: {e}\")\n",
    "        print(\"Mixed precision may not be supported on this GPU\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping mixed precision test (CUDA not available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0b1a93",
   "metadata": {},
   "source": [
    "## Test 6: Memory Management Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9460ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6Ô∏è‚É£ Memory Management Test\n",
      "Testing memory management patterns...\n",
      "Initial: 0.78 GB allocated, 0.79 GB reserved\n",
      "After creating tensors: 0.80 GB allocated, 0.82 GB reserved\n",
      "After deleting tensors: 0.78 GB allocated, 0.82 GB reserved\n",
      "After cache clear: 0.78 GB allocated, 0.82 GB reserved\n",
      "‚úÖ Memory management test complete\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n6Ô∏è‚É£ Memory Management Test\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    def get_memory_info():\n",
    "        allocated = torch.cuda.memory_allocated() / 1e9\n",
    "        reserved = torch.cuda.memory_reserved() / 1e9\n",
    "        return allocated, reserved\n",
    "    \n",
    "    print(\"Testing memory management patterns...\")\n",
    "    \n",
    "    # Initial state\n",
    "    alloc, res = get_memory_info()\n",
    "    print(f\"Initial: {alloc:.2f} GB allocated, {res:.2f} GB reserved\")\n",
    "    \n",
    "    # Create some tensors\n",
    "    tensors = []\n",
    "    for i in range(5):\n",
    "        tensor = torch.randn(100, 100, 100).cuda()  # ~40MB each\n",
    "        tensors.append(tensor)\n",
    "        \n",
    "    alloc, res = get_memory_info()\n",
    "    print(f\"After creating tensors: {alloc:.2f} GB allocated, {res:.2f} GB reserved\")\n",
    "    \n",
    "    # Delete tensors\n",
    "    del tensors\n",
    "    alloc, res = get_memory_info()\n",
    "    print(f\"After deleting tensors: {alloc:.2f} GB allocated, {res:.2f} GB reserved\")\n",
    "    \n",
    "    # Clear cache\n",
    "    torch.cuda.empty_cache()\n",
    "    alloc, res = get_memory_info()\n",
    "    print(f\"After cache clear: {alloc:.2f} GB allocated, {res:.2f} GB reserved\")\n",
    "    \n",
    "    print(\"‚úÖ Memory management test complete\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping memory management test (CUDA not available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fa42f3",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "406050fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üìä GPU Test Summary\n",
      "==================================================\n",
      "‚úÖ GPU Ready: NVIDIA GeForce RTX 5090\n",
      "‚úÖ Total VRAM: 34.2 GB\n",
      "‚úÖ PyTorch CUDA: 12.8\n",
      "\n",
      "üöÄ RTX 5090/4090 Detected - Excellent for LoRA training!\n",
      "   ‚Ä¢ Batch size: 4-8\n",
      "   ‚Ä¢ Mixed precision: Recommended\n",
      "   ‚Ä¢ 4-bit quantization: Optional\n",
      "\n",
      "üéØ Ready for LoRA training!\n",
      "\n",
      "üîß To install missing dependencies:\n",
      "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
      "pip install transformers accelerate bitsandbytes\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìä GPU Test Summary\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name()\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    \n",
    "    print(f\"‚úÖ GPU Ready: {gpu_name}\")\n",
    "    print(f\"‚úÖ Total VRAM: {total_memory:.1f} GB\")\n",
    "    print(f\"‚úÖ PyTorch CUDA: {torch.version.cuda}\")\n",
    "    \n",
    "    # Recommendations based on GPU\n",
    "    if \"RTX 5090\" in gpu_name or \"RTX 4090\" in gpu_name:\n",
    "        print(f\"\\nüöÄ RTX 5090/4090 Detected - Excellent for LoRA training!\")\n",
    "        print(f\"   ‚Ä¢ Batch size: 4-8\")\n",
    "        print(f\"   ‚Ä¢ Mixed precision: Recommended\")\n",
    "        print(f\"   ‚Ä¢ 4-bit quantization: Optional\")\n",
    "        \n",
    "    elif \"L40S\" in gpu_name or \"A100\" in gpu_name or \"H100\" in gpu_name:\n",
    "        print(f\"\\nüöÄ Professional GPU Detected - Perfect for large-scale training!\")\n",
    "        print(f\"   ‚Ä¢ Batch size: 8-16\")  \n",
    "        print(f\"   ‚Ä¢ Mixed precision: Highly recommended\")\n",
    "        print(f\"   ‚Ä¢ Full precision training: Possible\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  Unknown GPU - Training should work but may be slower\")\n",
    "        print(f\"   ‚Ä¢ Start with small batch sizes\")\n",
    "        print(f\"   ‚Ä¢ Use 4-bit quantization if memory is limited\")\n",
    "    \n",
    "    print(f\"\\nüéØ Ready for LoRA training!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No GPU detected\")\n",
    "    print(\"   ‚Ä¢ Training will be very slow on CPU\")\n",
    "    print(\"   ‚Ä¢ Consider using Google Colab or cloud GPUs\")\n",
    "    print(\"   ‚Ä¢ Or check CUDA installation\")\n",
    "\n",
    "print(f\"\\nüîß To install missing dependencies:\")\n",
    "print(f\"pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\")\n",
    "print(f\"pip install transformers accelerate bitsandbytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a090bfbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç GPU Environment Test\n",
      "==================================================\n",
      "\n",
      "1Ô∏è‚É£ Basic CUDA Detection\n",
      "PyTorch version: 2.7.0+cu128\n",
      "CUDA available: True\n",
      "CUDA version: 12.8\n",
      "Number of GPUs: 1\n",
      "  GPU 0: NVIDIA GeForce RTX 5090\n",
      "    Total memory: 34.2 GB\n",
      "    Multiprocessors: 170\n",
      "    CUDA capability: 12.0\n",
      "\n",
      "2Ô∏è‚É£ GPU Memory Test\n",
      "Initial - Allocated: 0.00 GB, Reserved: 0.00 GB\n",
      "Testing large tensor allocation...\n",
      "After allocation - Allocated: 4.00 GB, Reserved: 4.00 GB\n",
      "‚úÖ Large tensor allocation successful\n",
      "\n",
      "3Ô∏è‚É£ Simple Training Test\n",
      "Testing CPU training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\deanv\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\Roaming\\pypoetry\\venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 250])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU - Time: 0.30s, Avg Loss: 2.1292\n",
      "Testing GPU training...\n",
      "GPU - Time: 0.12s, Avg Loss: 1.0425\n",
      "üöÄ GPU speedup: 2.5x\n",
      "\n",
      "4Ô∏è‚É£ Transformers Library Test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\deanv\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\Roaming\\pypoetry\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing small transformer model on GPU...\n",
      "Model device: cuda:0\n",
      "Input device: cuda:0\n",
      "Output shape: torch.Size([1, 12, 768])\n",
      "Output device: cuda:0\n",
      "‚úÖ Transformers GPU test successful\n",
      "\n",
      "5Ô∏è‚É£ Mixed Precision Test\n",
      "‚úÖ Mixed precision (AMP) test successful\n",
      "This will speed up training on modern GPUs\n",
      "\n",
      "6Ô∏è‚É£ Memory Management Test\n",
      "Testing memory management patterns...\n",
      "Initial: 0.52 GB allocated, 0.55 GB reserved\n",
      "After creating tensors: 0.54 GB allocated, 0.55 GB reserved\n",
      "After deleting tensors: 0.53 GB allocated, 0.55 GB reserved\n",
      "After cache clear: 0.53 GB allocated, 0.55 GB reserved\n",
      "‚úÖ Memory management test complete\n",
      "\n",
      "==================================================\n",
      "üìä GPU Test Summary\n",
      "==================================================\n",
      "‚úÖ GPU Ready: NVIDIA GeForce RTX 5090\n",
      "‚úÖ Total VRAM: 34.2 GB\n",
      "‚úÖ PyTorch CUDA: 12.8\n",
      "\n",
      "üöÄ RTX 5090/4090 Detected - Excellent for LoRA training!\n",
      "   ‚Ä¢ Batch size: 4-8\n",
      "   ‚Ä¢ Mixed precision: Recommended\n",
      "   ‚Ä¢ 4-bit quantization: Optional\n",
      "\n",
      "üéØ Ready for LoRA training!\n",
      "\n",
      "üîß To install missing dependencies:\n",
      "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
      "pip install transformers accelerate bitsandbytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deanv\\AppData\\Local\\Temp\\ipykernel_96740\\1397077190.py:222: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "C:\\Users\\deanv\\AppData\\Local\\Temp\\ipykernel_96740\\1397077190.py:229: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "C:\\Users\\deanv\\AppData\\Local\\Temp\\ipykernel_96740\\1397077190.py:231: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 250])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(output, y)\n"
     ]
    }
   ],
   "source": [
    "# GPU Test Notebook - Verify Training Environment\n",
    "# Test GPU availability and basic training functionality\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "print(\"üîç GPU Environment Test\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "## Test 1: Basic GPU Detection\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ Basic CUDA Detection\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"  GPU {i}: {props.name}\")\n",
    "        print(f\"    Total memory: {props.total_memory / 1e9:.1f} GB\")\n",
    "        print(f\"    Multiprocessors: {props.multi_processor_count}\")\n",
    "        print(f\"    CUDA capability: {props.major}.{props.minor}\")\n",
    "else:\n",
    "    print(\"‚ùå CUDA not available - training will be slow!\")\n",
    "\n",
    "## Test 2: Memory Test\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ GPU Memory Test\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    \n",
    "    # Clear cache\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Check initial memory\n",
    "    allocated = torch.cuda.memory_allocated() / 1e9\n",
    "    reserved = torch.cuda.memory_reserved() / 1e9\n",
    "    print(f\"Initial - Allocated: {allocated:.2f} GB, Reserved: {reserved:.2f} GB\")\n",
    "    \n",
    "    # Test large tensor allocation\n",
    "    try:\n",
    "        print(\"Testing large tensor allocation...\")\n",
    "        test_tensor = torch.randn(1000, 1000, 1000, device=device)  # ~4GB\n",
    "        \n",
    "        allocated = torch.cuda.memory_allocated() / 1e9\n",
    "        reserved = torch.cuda.memory_reserved() / 1e9\n",
    "        print(f\"After allocation - Allocated: {allocated:.2f} GB, Reserved: {reserved:.2f} GB\")\n",
    "        \n",
    "        # Clean up\n",
    "        del test_tensor\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"‚úÖ Large tensor allocation successful\")\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        print(f\"‚ùå Memory allocation failed: {e}\")\n",
    "        print(\"This might indicate insufficient GPU memory\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping GPU memory test (CUDA not available)\")\n",
    "\n",
    "## Test 3: Simple Neural Network Training\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ Simple Training Test\")\n",
    "\n",
    "# Create a simple neural network\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(1000, 5000)\n",
    "        self.fc2 = nn.Linear(5000, 5000)\n",
    "        self.fc3 = nn.Linear(5000, 250)\n",
    "        self.fc4 = nn.Linear(250, 25)\n",
    "        self.fc5 = nn.Linear(25, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "# Generate dummy data\n",
    "X = torch.randn(1000, 1000)\n",
    "y = torch.randn(1000, 1)\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TensorDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Initialize model\n",
    "model = SimpleNet()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Test CPU training first\n",
    "print(\"Testing CPU training...\")\n",
    "device = torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "start_time = time.time()\n",
    "model.train()\n",
    "total_loss = 0\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(dataloader):\n",
    "    if batch_idx >= 5:  # Only test 5 batches\n",
    "        break\n",
    "        \n",
    "    data, target = data.to(device), target.to(device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    total_loss += loss.item()\n",
    "\n",
    "cpu_time = time.time() - start_time\n",
    "cpu_loss = total_loss / 5\n",
    "print(f\"CPU - Time: {cpu_time:.2f}s, Avg Loss: {cpu_loss:.4f}\")\n",
    "\n",
    "# Test GPU training (if available)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Testing GPU training...\")\n",
    "    device = torch.device(\"cuda\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Need to recreate optimizer after moving model to GPU\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        if batch_idx >= 5:  # Only test 5 batches\n",
    "            break\n",
    "            \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    gpu_time = time.time() - start_time\n",
    "    gpu_loss = total_loss / 5\n",
    "    print(f\"GPU - Time: {gpu_time:.2f}s, Avg Loss: {gpu_loss:.4f}\")\n",
    "    \n",
    "    if cpu_time > 0:\n",
    "        speedup = cpu_time / gpu_time\n",
    "        print(f\"üöÄ GPU speedup: {speedup:.1f}x\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping GPU training test (CUDA not available)\")\n",
    "\n",
    "## Test 4: Transformers Library GPU Test\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£ Transformers Library Test\")\n",
    "\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "    import torch.nn.functional as F\n",
    "    \n",
    "    print(\"Testing small transformer model on GPU...\")\n",
    "    \n",
    "    # Use a small model for testing\n",
    "    model_name = \"distilbert-base-uncased\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    \n",
    "    # Test input\n",
    "    text = \"This is a test sentence for GPU processing.\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        model = model.to(device)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        print(f\"Model device: {next(model.parameters()).device}\")\n",
    "        print(f\"Input device: {inputs['input_ids'].device}\")\n",
    "        \n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        print(f\"Output shape: {outputs.last_hidden_state.shape}\")\n",
    "        print(f\"Output device: {outputs.last_hidden_state.device}\")\n",
    "        print(\"‚úÖ Transformers GPU test successful\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚è≠Ô∏è Skipping transformers GPU test (CUDA not available)\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"‚ùå Transformers library not installed\")\n",
    "    print(\"Install with: pip install transformers\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Transformers test failed: {e}\")\n",
    "\n",
    "## Test 5: Mixed Precision Test (for training efficiency)\n",
    "\n",
    "print(\"\\n5Ô∏è‚É£ Mixed Precision Test\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    try:\n",
    "        from torch.cuda.amp import autocast, GradScaler\n",
    "        \n",
    "        # Create model and data\n",
    "        model = SimpleNet().cuda()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        scaler = GradScaler()\n",
    "        \n",
    "        # Test data\n",
    "        x = torch.randn(32, 1000).cuda()\n",
    "        y = torch.randn(32, 1).cuda()\n",
    "        \n",
    "        # Mixed precision forward pass\n",
    "        with autocast():\n",
    "            output = model(x)\n",
    "            loss = F.mse_loss(output, y)\n",
    "        \n",
    "        # Backward pass with scaling\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        print(\"‚úÖ Mixed precision (AMP) test successful\")\n",
    "        print(\"This will speed up training on modern GPUs\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Mixed precision test failed: {e}\")\n",
    "        print(\"Mixed precision may not be supported on this GPU\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping mixed precision test (CUDA not available)\")\n",
    "\n",
    "## Test 6: Memory Management Test\n",
    "\n",
    "print(\"\\n6Ô∏è‚É£ Memory Management Test\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    def get_memory_info():\n",
    "        allocated = torch.cuda.memory_allocated() / 1e9\n",
    "        reserved = torch.cuda.memory_reserved() / 1e9\n",
    "        return allocated, reserved\n",
    "    \n",
    "    print(\"Testing memory management patterns...\")\n",
    "    \n",
    "    # Initial state\n",
    "    alloc, res = get_memory_info()\n",
    "    print(f\"Initial: {alloc:.2f} GB allocated, {res:.2f} GB reserved\")\n",
    "    \n",
    "    # Create some tensors\n",
    "    tensors = []\n",
    "    for i in range(5):\n",
    "        tensor = torch.randn(100, 100, 100).cuda()  # ~40MB each\n",
    "        tensors.append(tensor)\n",
    "        \n",
    "    alloc, res = get_memory_info()\n",
    "    print(f\"After creating tensors: {alloc:.2f} GB allocated, {res:.2f} GB reserved\")\n",
    "    \n",
    "    # Delete tensors\n",
    "    del tensors\n",
    "    alloc, res = get_memory_info()\n",
    "    print(f\"After deleting tensors: {alloc:.2f} GB allocated, {res:.2f} GB reserved\")\n",
    "    \n",
    "    # Clear cache\n",
    "    torch.cuda.empty_cache()\n",
    "    alloc, res = get_memory_info()\n",
    "    print(f\"After cache clear: {alloc:.2f} GB allocated, {res:.2f} GB reserved\")\n",
    "    \n",
    "    print(\"‚úÖ Memory management test complete\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping memory management test (CUDA not available)\")\n",
    "\n",
    "## Summary\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìä GPU Test Summary\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name()\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    \n",
    "    print(f\"‚úÖ GPU Ready: {gpu_name}\")\n",
    "    print(f\"‚úÖ Total VRAM: {total_memory:.1f} GB\")\n",
    "    print(f\"‚úÖ PyTorch CUDA: {torch.version.cuda}\")\n",
    "    \n",
    "    # Recommendations based on GPU\n",
    "    if \"RTX 5090\" in gpu_name or \"RTX 4090\" in gpu_name:\n",
    "        print(f\"\\nüöÄ RTX 5090/4090 Detected - Excellent for LoRA training!\")\n",
    "        print(f\"   ‚Ä¢ Batch size: 4-8\")\n",
    "        print(f\"   ‚Ä¢ Mixed precision: Recommended\")\n",
    "        print(f\"   ‚Ä¢ 4-bit quantization: Optional\")\n",
    "        \n",
    "    elif \"L40S\" in gpu_name or \"A100\" in gpu_name or \"H100\" in gpu_name:\n",
    "        print(f\"\\nüöÄ Professional GPU Detected - Perfect for large-scale training!\")\n",
    "        print(f\"   ‚Ä¢ Batch size: 8-16\")  \n",
    "        print(f\"   ‚Ä¢ Mixed precision: Highly recommended\")\n",
    "        print(f\"   ‚Ä¢ Full precision training: Possible\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  Unknown GPU - Training should work but may be slower\")\n",
    "        print(f\"   ‚Ä¢ Start with small batch sizes\")\n",
    "        print(f\"   ‚Ä¢ Use 4-bit quantization if memory is limited\")\n",
    "    \n",
    "    print(f\"\\nüéØ Ready for LoRA training!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No GPU detected\")\n",
    "    print(\"   ‚Ä¢ Training will be very slow on CPU\")\n",
    "    print(\"   ‚Ä¢ Consider using Google Colab or cloud GPUs\")\n",
    "    print(\"   ‚Ä¢ Or check CUDA installation\")\n",
    "\n",
    "print(f\"\\nüîß To install missing dependencies:\")\n",
    "print(f\"pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\")\n",
    "print(f\"pip install transformers accelerate bitsandbytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb840aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

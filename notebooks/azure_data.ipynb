{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17f9319f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Datastore\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "from azureml.core import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46c4aa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_id = \"0a94de80-6d3b-49f2-b3e9-ec5818862801\"\n",
    "resource_group = \"buas-y2\"\n",
    "workspace_name = \"Test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e09ec394",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = InteractiveLoginAuthentication()\n",
    "workspace = Workspace(subscription_id=subscription_id,\n",
    "                   resource_group=resource_group,\n",
    "                   workspace_name=workspace_name,\n",
    "                   auth=auth,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c41f8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workspaceartifactstore AzureBlob\n",
      "workspaceblobstore AzureBlob\n",
      "workspaceworkingdirectory AzureFile\n"
     ]
    }
   ],
   "source": [
    "# list all datastores registered in the current workspace\n",
    "datastores = workspace.datastores\n",
    "for name, datastore in datastores.items():\n",
    "    print(name, datastore.datastore_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e67a3625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 14 files\n",
      "Uploading test_data\\train\\chunk_01_segments.csv\n",
      "Uploaded test_data\\train\\chunk_01_segments.csv, 1 files out of an estimated total of 14\n",
      "Uploading test_data\\train\\chunk_03_segments.csv\n",
      "Uploaded test_data\\train\\chunk_03_segments.csv, 2 files out of an estimated total of 14\n",
      "Uploading test_data\\train\\chunk_05_segments.csv\n",
      "Uploaded test_data\\train\\chunk_05_segments.csv, 3 files out of an estimated total of 14\n",
      "Uploading test_data\\train\\chunk_07_segments.csv\n",
      "Uploaded test_data\\train\\chunk_07_segments.csv, 4 files out of an estimated total of 14\n",
      "Uploading test_data\\train\\chunk_08_segments.csv\n",
      "Uploaded test_data\\train\\chunk_08_segments.csv, 5 files out of an estimated total of 14\n",
      "Uploading test_data\\validate\\chunk_14_segments.csv\n",
      "Uploaded test_data\\validate\\chunk_14_segments.csv, 6 files out of an estimated total of 14\n",
      "Uploading test_data\\test\\chunk_11_segments.csv\n",
      "Uploaded test_data\\test\\chunk_11_segments.csv, 7 files out of an estimated total of 14\n",
      "Uploading test_data\\test\\chunk_12_segments.csv\n",
      "Uploaded test_data\\test\\chunk_12_segments.csv, 8 files out of an estimated total of 14\n",
      "Uploading test_data\\test\\chunk_13_segments.csv\n",
      "Uploaded test_data\\test\\chunk_13_segments.csv, 9 files out of an estimated total of 14\n",
      "Uploading test_data\\train\\chunk_02_segments.csv\n",
      "Uploaded test_data\\train\\chunk_02_segments.csv, 10 files out of an estimated total of 14\n",
      "Uploading test_data\\train\\chunk_04_segments.csv\n",
      "Uploaded test_data\\train\\chunk_04_segments.csv, 11 files out of an estimated total of 14\n",
      "Uploading test_data\\train\\chunk_06_segments.csv\n",
      "Uploaded test_data\\train\\chunk_06_segments.csv, 12 files out of an estimated total of 14\n",
      "Uploading test_data\\validate\\chunk_09_segments.csv\n",
      "Uploaded test_data\\validate\\chunk_09_segments.csv, 13 files out of an estimated total of 14\n",
      "Uploading test_data\\validate\\chunk_10_segments.csv\n",
      "Uploaded test_data\\validate\\chunk_10_segments.csv, 14 files out of an estimated total of 14\n",
      "Uploaded 14 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_67d7459c65ae4e0192a63805daba6ae3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a datastore object from the existing datastore named \"workspaceblobstore\".\n",
    "datastore = Datastore(workspace, name='workspaceblobstore')\n",
    "\n",
    "# Upload the data to the path target_path in datastore\n",
    "datastore.upload(src_dir='test_data', target_path='transcripts', overwrite=True, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4b135c",
   "metadata": {},
   "source": [
    "# This is nice with SDKV1 but doesnt play nice with NEMO. The SDK2 way is way too complicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b32b2a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-25 13:51:19.189271 | ActivityCompleted: Activity=from_files, HowEnded=Failure, Duration=0.0 [ms], Info = {'activity_id': 'e1620620-bfd9-456b-b432-80bdc0dd4daa', 'activity_name': 'from_files', 'activity_type': 'PublicApi', 'app_name': 'FileDataset', 'source': 'azureml.dataset', 'version': '1.60.0', 'dataprepVersion': '', 'sparkVersion': '', 'subscription': '', 'run_id': '', 'resource_group': '', 'workspace_name': '', 'experiment_id': '', 'location': ''}, Exception=ImportError; Missing required package \"azureml-dataset-runtime\", which can be installed by running: \"c:\\Users\\deanv\\Dropbox\\0_Buas\\2024-2025\\Y2D\\Demo Project\\mlops-demo-project\\backend-microservice\\.venv\\Scripts\\python.exe\" -m pip install azureml-dataset-runtime --upgrade\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Missing required package \"azureml-dataset-runtime\", which can be installed by running: \"c:\\Users\\deanv\\Dropbox\\0_Buas\\2024-2025\\Y2D\\Demo Project\\mlops-demo-project\\backend-microservice\\.venv\\Scripts\\python.exe\" -m pip install azureml-dataset-runtime --upgrade",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create a FileDataset from a path to a directory.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# The directory contains a folder per class, each of which contains image files.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m sample_set = \u001b[43mDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mFile\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatastore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtranscripts\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m paths = sample_set.take_sample(\u001b[32m0.001\u001b[39m).take(\u001b[32m30\u001b[39m).download()\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(paths)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\deanv\\Dropbox\\0_Buas\\2024-2025\\Y2D\\Demo Project\\mlops-demo-project\\backend-microservice\\.venv\\Lib\\site-packages\\azureml\\data\\_loggerfactory.py:140\u001b[39m, in \u001b[36mtrack.<locals>.monitor.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _LoggerFactory.track_activity(logger, func.\u001b[34m__name__\u001b[39m, activity_type, custom_dimensions) \u001b[38;5;28;01mas\u001b[39;00m al:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    142\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(al, \u001b[33m'\u001b[39m\u001b[33mactivity_info\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[33m'\u001b[39m\u001b[33merror_code\u001b[39m\u001b[33m'\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\deanv\\Dropbox\\0_Buas\\2024-2025\\Y2D\\Demo Project\\mlops-demo-project\\backend-microservice\\.venv\\Lib\\site-packages\\azureml\\data\\dataset_factory.py:881\u001b[39m, in \u001b[36mFileDatasetFactory.from_files\u001b[39m\u001b[34m(path, validate, partition_format, is_file)\u001b[39m\n\u001b[32m    878\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_file \u001b[38;5;129;01mand\u001b[39;00m validate:\n\u001b[32m    879\u001b[39m     warnings.warn(\u001b[33m\"\u001b[39m\u001b[33mNo validation will be performed because `is_file` is set to True.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m881\u001b[39m dataflow = \u001b[43mdataprep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.EnginelessDataflow.from_paths(path, is_file)\n\u001b[32m    883\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m partition_format:\n\u001b[32m    884\u001b[39m     dataflow = _transform_and_validate(dataflow, partition_format, validate)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\deanv\\Dropbox\\0_Buas\\2024-2025\\Y2D\\Demo Project\\mlops-demo-project\\backend-microservice\\.venv\\Lib\\site-packages\\azureml\\data\\_dataprep_helper.py:36\u001b[39m, in \u001b[36mdataprep\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdataprep\u001b[39m():\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dataprep_installed():\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(get_dataprep_missing_message())\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mazureml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataprep\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_dprep\u001b[39;00m\n\u001b[32m     38\u001b[39m     check_min_version()\n",
      "\u001b[31mImportError\u001b[39m: Missing required package \"azureml-dataset-runtime\", which can be installed by running: \"c:\\Users\\deanv\\Dropbox\\0_Buas\\2024-2025\\Y2D\\Demo Project\\mlops-demo-project\\backend-microservice\\.venv\\Scripts\\python.exe\" -m pip install azureml-dataset-runtime --upgrade"
     ]
    }
   ],
   "source": [
    "# Create a FileDataset from a path to a directory.\n",
    "# The directory contains a folder per class, each of which contains image files.\n",
    "sample_set = Dataset.File.from_files(path=(datastore, 'transcripts'))\n",
    "paths = sample_set.take_sample(0.001).take(30).download()\n",
    "print(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1a620ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-25 14:02:55.082611 | ActivityCompleted: Activity=from_files, HowEnded=Failure, Duration=0.0 [ms], Info = {'activity_id': '444fff19-5219-4594-a6d7-a8cf31e2a0bc', 'activity_name': 'from_files', 'activity_type': 'PublicApi', 'app_name': 'FileDataset', 'source': 'azureml.dataset', 'version': '1.60.0', 'dataprepVersion': '', 'sparkVersion': '', 'subscription': '', 'run_id': '', 'resource_group': '', 'workspace_name': '', 'experiment_id': '', 'location': ''}, Exception=ImportError; Missing required package \"azureml-dataset-runtime\", which can be installed by running: \"c:\\Users\\deanv\\Dropbox\\0_Buas\\2024-2025\\Y2D\\Demo Project\\mlops-demo-project\\backend-microservice\\.venv\\Scripts\\python.exe\" -m pip install azureml-dataset-runtime --upgrade\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Missing required package \"azureml-dataset-runtime\", which can be installed by running: \"c:\\Users\\deanv\\Dropbox\\0_Buas\\2024-2025\\Y2D\\Demo Project\\mlops-demo-project\\backend-microservice\\.venv\\Scripts\\python.exe\" -m pip install azureml-dataset-runtime --upgrade",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create a FileDataset from a path to a directory for the training data.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m train_set = \u001b[43mDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mFile\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatastore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtranscripts/train\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Split the dataset into train and validation sets\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# train_set, val_set = train_set.random_split(0.8, seed=123)\u001b[39;00m\n\u001b[32m      5\u001b[39m val_set = Dataset.File.from_files(path=(datastore, \u001b[33m'\u001b[39m\u001b[33mtranscripts/train\u001b[39m\u001b[33m'\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\deanv\\Dropbox\\0_Buas\\2024-2025\\Y2D\\Demo Project\\mlops-demo-project\\backend-microservice\\.venv\\Lib\\site-packages\\azureml\\data\\_loggerfactory.py:140\u001b[39m, in \u001b[36mtrack.<locals>.monitor.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _LoggerFactory.track_activity(logger, func.\u001b[34m__name__\u001b[39m, activity_type, custom_dimensions) \u001b[38;5;28;01mas\u001b[39;00m al:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    142\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(al, \u001b[33m'\u001b[39m\u001b[33mactivity_info\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[33m'\u001b[39m\u001b[33merror_code\u001b[39m\u001b[33m'\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\deanv\\Dropbox\\0_Buas\\2024-2025\\Y2D\\Demo Project\\mlops-demo-project\\backend-microservice\\.venv\\Lib\\site-packages\\azureml\\data\\dataset_factory.py:881\u001b[39m, in \u001b[36mFileDatasetFactory.from_files\u001b[39m\u001b[34m(path, validate, partition_format, is_file)\u001b[39m\n\u001b[32m    878\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_file \u001b[38;5;129;01mand\u001b[39;00m validate:\n\u001b[32m    879\u001b[39m     warnings.warn(\u001b[33m\"\u001b[39m\u001b[33mNo validation will be performed because `is_file` is set to True.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m881\u001b[39m dataflow = \u001b[43mdataprep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.EnginelessDataflow.from_paths(path, is_file)\n\u001b[32m    883\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m partition_format:\n\u001b[32m    884\u001b[39m     dataflow = _transform_and_validate(dataflow, partition_format, validate)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\deanv\\Dropbox\\0_Buas\\2024-2025\\Y2D\\Demo Project\\mlops-demo-project\\backend-microservice\\.venv\\Lib\\site-packages\\azureml\\data\\_dataprep_helper.py:36\u001b[39m, in \u001b[36mdataprep\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdataprep\u001b[39m():\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dataprep_installed():\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(get_dataprep_missing_message())\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mazureml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataprep\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_dprep\u001b[39;00m\n\u001b[32m     38\u001b[39m     check_min_version()\n",
      "\u001b[31mImportError\u001b[39m: Missing required package \"azureml-dataset-runtime\", which can be installed by running: \"c:\\Users\\deanv\\Dropbox\\0_Buas\\2024-2025\\Y2D\\Demo Project\\mlops-demo-project\\backend-microservice\\.venv\\Scripts\\python.exe\" -m pip install azureml-dataset-runtime --upgrade"
     ]
    }
   ],
   "source": [
    "# Create a FileDataset from a path to a directory for the training data.\n",
    "train_set = Dataset.File.from_files(path=(datastore, 'transcripts/train'))\n",
    "# Split the dataset into train and validation sets\n",
    "# train_set, val_set = train_set.random_split(0.8, seed=123)\n",
    "val_set = Dataset.File.from_files(path=(datastore, 'transcripts/train'))\n",
    "# Create a FileDataset from a path to a directory for the test data.\n",
    "test_set = Dataset.File.from_files(path=(datastore, 'transcripts/test'))\n",
    "\n",
    "#register the datasets\n",
    "train_reg = train_set.register(workspace=workspace, name='transcripts_train', description='training data', create_new_version=True)\n",
    "val_reg = val_set.register(workspace=workspace, name='transcripts_val', description='validation data', create_new_version=True)\n",
    "test_reg = test_set.register(workspace=workspace, name='transcriptss_test', description='test data', create_new_version=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "568e58b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.identity import InteractiveBrowserCredential\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dcd7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize client\n",
    "ml_client = MLClient(\n",
    "    credential=InteractiveBrowserCredential(),\n",
    "    subscription_id=subscription_id,\n",
    "    resource_group_name=resource_group,\n",
    "    workspace_name=workspace_name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "87486cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get datastore reference\n",
    "datastore = ml_client.datastores.get(\"workspaceblobstore\")  # or your datastore name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bbc081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data URIs (equivalent to Dataset.File.from_files)\n",
    "train_uri = f\"azureml://datastores/{datastore.name}/paths/transcript/train\"\n",
    "val_uri = f\"azureml://datastores/{datastore.name}/paths/transcript/validaion\"\n",
    "test_uri = f\"azureml://datastores/{datastore.name}/paths/transcript/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "86d1d4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered datasets:\n",
      "Train: transcripts_train v1\n",
      "Val: transcripts_val v1\n",
      "Test: transcripts_test v1\n"
     ]
    }
   ],
   "source": [
    "train_data = Data(\n",
    "    name=\"transcripts_train\",\n",
    "    description=\"training data\",\n",
    "    path=train_uri,\n",
    "    type=AssetTypes.URI_FOLDER\n",
    ")\n",
    "\n",
    "val_data = Data(\n",
    "    name=\"transcripts_val\",\n",
    "    description=\"validation data\",\n",
    "    path=val_uri,\n",
    "    type=AssetTypes.URI_FOLDER\n",
    ")\n",
    "\n",
    "test_data = Data(\n",
    "    name=\"transcripts_test\",\n",
    "    description=\"test data\", \n",
    "    path=test_uri,\n",
    "    type=AssetTypes.URI_FOLDER\n",
    ")\n",
    "\n",
    "# Register the datasets\n",
    "train_reg = ml_client.data.create_or_update(train_data)\n",
    "val_reg = ml_client.data.create_or_update(val_data)\n",
    "test_reg = ml_client.data.create_or_update(test_data)\n",
    "\n",
    "print(f\"Registered datasets:\")\n",
    "print(f\"Train: {train_reg.name} v{train_reg.version}\")\n",
    "print(f\"Val: {val_reg.name} v{val_reg.version}\")\n",
    "print(f\"Test: {test_reg.name} v{test_reg.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c5769258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all datasets registered in the current workspace\n",
    "datasets = workspace.datasets\n",
    "for name, dataset in datasets.items():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7a62c420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All registered data assets:\n",
      "--------------------------------------------------\n",
      "📊 transcripts_train\n",
      "   Version: None\n",
      "   Type: uri_folder\n",
      "   Description: None\n",
      "   Path: None\n",
      "   Created: 2025-05-25 14:22:06.376834+00:00\n",
      "\n",
      "📊 transcripts_val\n",
      "   Version: None\n",
      "   Type: uri_folder\n",
      "   Description: None\n",
      "   Path: None\n",
      "   Created: 2025-05-25 14:22:07.623089+00:00\n",
      "\n",
      "📊 transcripts_test\n",
      "   Version: None\n",
      "   Type: uri_folder\n",
      "   Description: None\n",
      "   Path: None\n",
      "   Created: 2025-05-25 14:22:08.531985+00:00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List all data assets\n",
    "print(\"All registered data assets:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for data_asset in ml_client.data.list():\n",
    "    print(f\"📊 {data_asset.name}\")\n",
    "    print(f\"   Version: {data_asset.version}\")\n",
    "    print(f\"   Type: {data_asset.type}\")\n",
    "    print(f\"   Description: {data_asset.description}\")\n",
    "    print(f\"   Path: {data_asset.path}\")\n",
    "    print(f\"   Created: {data_asset.creation_context.created_at}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f888fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfad92c3",
   "metadata": {},
   "source": [
    "# D&D Summarization LoRA Training - Simple Case\n",
    "# Training Llama 3.1 1B on a few transcript/summary pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c81cb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\deanv\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\Roaming\\pypoetry\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer\n",
    "import dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019072f1",
   "metadata": {},
   "source": [
    "# Check GPU availability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbd76397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 5090\n",
      "VRAM: 34.2 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17fde4e",
   "metadata": {},
   "source": [
    "## Step 1: Load and Prepare Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2ff8f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data(summaries_dir=\"../data/summaries\", transcripts_dir=\"../data/combined_transcripts\"):\n",
    "    \"\"\"Load the 14 training pairs from your summary JSON files.\"\"\"\n",
    "    print(f\"Loading training data from {summaries_dir}\")\n",
    "    \n",
    "    summaries_path = Path(summaries_dir)\n",
    "    # print top 5 files in the directory\n",
    "    print(\"Files in summaries directory:\")\n",
    "    for file in sorted(summaries_path.glob(\"chunk_*_summary.json\"))[:5]:\n",
    "        print(f\" - {file.name}\")\n",
    "\n",
    "    training_pairs = []\n",
    "    \n",
    "    # Load all summary files\n",
    "    for json_file in sorted(summaries_path.glob(\"chunk_*_summary.json\")):\n",
    "        print(f\"Processing {json_file.name}\")\n",
    "        try:\n",
    "            with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Extract the combined transcript (this will be our input)\n",
    "            # You'll need to load the corresponding simple.json file for the transcript\n",
    "            chunk_num = data['chunk_number']\n",
    "            simple_file = summaries_path.parent / transcripts_dir / f\"chunk_{chunk_num:02d}_simple.json\"\n",
    "            \n",
    "            if simple_file.exists():\n",
    "                with open(simple_file, 'r', encoding='utf-8') as f:\n",
    "                    chunk_data = json.load(f)\n",
    "                \n",
    "                training_pair = {\n",
    "                    'chunk_id': f\"chunk_{chunk_num:02d}\",\n",
    "                    'input_text': chunk_data['combined_transcript'],\n",
    "                    'target_summary': data['summary'],\n",
    "                    'duration_minutes': data['duration_minutes'],\n",
    "                    'word_count': data['word_count']\n",
    "                }\n",
    "                training_pairs.append(training_pair)            \n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {json_file}: {e}\")\n",
    "\n",
    "    print(f\"Loaded {len(training_pairs)} training pairs\")\n",
    "    return training_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22587693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data from ../data/summaries\n",
      "Files in summaries directory:\n",
      " - chunk_01_summary.json\n",
      " - chunk_02_summary.json\n",
      " - chunk_03_summary.json\n",
      " - chunk_04_summary.json\n",
      " - chunk_05_summary.json\n",
      "Processing chunk_01_summary.json\n",
      "Processing chunk_02_summary.json\n",
      "Processing chunk_03_summary.json\n",
      "Processing chunk_04_summary.json\n",
      "Processing chunk_05_summary.json\n",
      "Processing chunk_06_summary.json\n",
      "Processing chunk_07_summary.json\n",
      "Processing chunk_08_summary.json\n",
      "Processing chunk_09_summary.json\n",
      "Processing chunk_10_summary.json\n",
      "Processing chunk_11_summary.json\n",
      "Processing chunk_12_summary.json\n",
      "Processing chunk_13_summary.json\n",
      "Processing chunk_14_summary.json\n",
      "Loaded 14 training pairs\n"
     ]
    }
   ],
   "source": [
    "# Load your data\n",
    "training_data = load_training_data(transcripts_dir=\"../data/combined_transcripts_20min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2108e30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 Example Training Pair:\n",
      "Chunk: chunk_01\n",
      "Duration: 20.0 minutes\n",
      "Input length: 1191 chars\n",
      "Summary length: 3161 chars\n",
      "\n",
      "Input preview: \\n\\n=== File 1: Critical Role plays Daggerheart ｜ Live One-Shot ｜ Open Beta_chunk_0_300_seconds (t=0.0s) ===\\n[0.0s - 300.0s] \\n\\n\\n=== File 2: Critical Role plays Daggerheart ｜ Live One-Shot ｜ Open B...\n",
      "\n",
      "Target summary preview: In this thrilling 20-minute session of Critical Role's Daggerheart one-shot, the party embarked on an exhilarating adventure filled with mystery and magic. As they celebrated their nine-year anniversa...\n"
     ]
    }
   ],
   "source": [
    "# Display first example\n",
    "\n",
    "example = training_data[0]\n",
    "print(f\"\\n📋 Example Training Pair:\")\n",
    "print(f\"Chunk: {example['chunk_id']}\")\n",
    "print(f\"Duration: {example['duration_minutes']:.1f} minutes\")\n",
    "print(f\"Input length: {len(example['input_text'])} chars\")\n",
    "print(f\"Summary length: {len(example['target_summary'])} chars\")\n",
    "print(f\"\\nInput preview: {example['input_text'][:200]}...\")\n",
    "print(f\"\\nTarget summary preview: {example['target_summary'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a5381f",
   "metadata": {},
   "source": [
    "## Step 2: Setup Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91a47371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🤖 Loading model: meta-llama/Llama-3.2-1B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\deanv\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\Roaming\\pypoetry\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\deanv\\.cache\\huggingface\\hub\\models--meta-llama--Llama-3.2-1B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"meta-llama/Llama-3.2-1B-Instruct\"  # or \"meta-llama/Llama-3.2-1B\"\n",
    "# MODEL_NAME = \"microsoft/DialoGPT-medium\" \n",
    "hf_token = os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "\n",
    "print(f\"\\n🤖 Loading model: {MODEL_NAME}\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    token=hf_token \n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6a2470e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\deanv\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\Roaming\\pypoetry\\venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# # Load model in 4-bit for memory efficiency (optional, remove if you have enough VRAM)\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,  # Remove this line if you want full precision\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    use_auth_token=hf_token  # Use your Hugging Face token for private models\n",
    ")\n",
    "\n",
    "print(f\"Model loaded on: {model.device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bcc3ee",
   "metadata": {},
   "source": [
    "## Step 3: Configure LoRA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9aa3fd80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 11,272,192 || all params: 1,247,086,592 || trainable%: 0.9039\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=16,                    # Rank - start small for 14 samples\n",
    "    lora_alpha=32,          # Scaling parameter\n",
    "    lora_dropout=0.1,       # Dropout for regularization\n",
    "    target_modules=[        # Target attention modules\n",
    "        \"q_proj\",\n",
    "        \"k_proj\", \n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",        # Also target MLP for better performance\n",
    "        \"up_proj\",\n",
    "        \"down_proj\"\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    use_rslora=False,       # Set to True for better stability with larger ranks\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581e7df9",
   "metadata": {},
   "source": [
    "## Step 4: Create Training Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aea2e655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_prompt(input_text, target_summary):\n",
    "    \"\"\"Create a formatted prompt for training.\"\"\"\n",
    "    prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are an expert at summarizing Dungeons & Dragons sessions. Create engaging, detailed summaries that capture the story progression, character moments, combat encounters, and future plot hooks.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Summarize this D&D session transcript in 300-500 words:\n",
    "\n",
    "{input_text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{target_summary}<|eot_id|>\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78457093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize the training examples.\"\"\"\n",
    "    # Create full prompts\n",
    "    prompts = [create_training_prompt(inp, target) \n",
    "               for inp, target in zip(examples['input_text'], examples['target_summary'])]\n",
    "    \n",
    "    # Tokenize with truncation for long sequences\n",
    "    tokenized = tokenizer(\n",
    "        prompts,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=2048,#4096,  # Adjust based on your GPU memory\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    # For causal LM, labels are the same as input_ids\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    \n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c58c525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔤 Tokenizing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 14/14 [00:00<00:00, 283.50 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 14\n",
      "Average sequence length: 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert to HuggingFace Dataset\n",
    "df = pd.DataFrame(training_data)\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Tokenize dataset\n",
    "print(\"\\n🔤 Tokenizing dataset...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "\n",
    "print(f\"Dataset size: {len(tokenized_dataset)}\")\n",
    "print(f\"Average sequence length: {np.mean([len(x) for x in tokenized_dataset['input_ids']]):.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed150bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 11\n",
      "Validation samples: 3\n"
     ]
    }
   ],
   "source": [
    "# Split into train/validation (with only 14 samples, we'll use a simple split)\n",
    "train_size = int(0.8 * len(tokenized_dataset))  # 11 for training, 3 for validation\n",
    "train_dataset = tokenized_dataset.select(range(train_size))\n",
    "eval_dataset = tokenized_dataset.select(range(train_size, len(tokenized_dataset)))\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9a212c",
   "metadata": {},
   "source": [
    "## Step 5: Training Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36614fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./dnd_lora_checkpoints\",\n",
    "    num_train_epochs=10,              # More epochs for small dataset\n",
    "    per_device_train_batch_size=4,   # Small batch due to long sequences\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,   # Accumulate gradients to simulate larger batch size, effectively 8 per step\n",
    "    learning_rate=2e-4,              # Standard LoRA learning rate\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=1,                 # Log every step for small dataset\n",
    "    eval_strategy=\"epoch\",     # Evaluate after each epoch\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    warmup_steps=2,                  # Small warmup for small dataset\n",
    "    fp16=False,                      # Use bf16 instead if supported\n",
    "    bf16=True,\n",
    "    dataloader_num_workers=0,        # Avoid multiprocessing issues\n",
    "    remove_unused_columns=False,\n",
    "    report_to=None,                  # Disable wandb/tensorboard for now\n",
    ")\n",
    "\n",
    "# Data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # We're doing causal LM, not masked LM\n",
    "    pad_to_multiple_of=8,  # Pad to multiple of 8 for better performance\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33003970",
   "metadata": {},
   "source": [
    "## Step 6: Initialize Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7ebc075",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deanv\\AppData\\Local\\Temp\\ipykernel_93464\\1518105968.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Starting training...\n",
      "Training 11 samples for 10 epochs\n",
      "Effective batch size: 8\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"\\n🚀 Starting training...\")\n",
    "print(f\"Training {len(train_dataset)} samples for {training_args.num_train_epochs} epochs\")\n",
    "print(f\"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28b3dc1",
   "metadata": {},
   "source": [
    "## Step 7: Train the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2dd309ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:48, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.247400</td>\n",
       "      <td>2.141451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.046800</td>\n",
       "      <td>2.016145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.935300</td>\n",
       "      <td>1.954644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.849900</td>\n",
       "      <td>1.908041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.790300</td>\n",
       "      <td>1.865223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.793500</td>\n",
       "      <td>1.836725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.620900</td>\n",
       "      <td>1.817530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.440600</td>\n",
       "      <td>1.806164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.617200</td>\n",
       "      <td>1.799960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.613100</td>\n",
       "      <td>1.798006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\deanv\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\Roaming\\pypoetry\\venv\\Lib\\site-packages\\peft\\utils\\other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6832c58d-2b5956405103913d19c8cb19;f2536feb-ac3a-4271-88f4-964cf26bf633)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B-Instruct.\n",
      "  warnings.warn(\n",
      "c:\\Users\\deanv\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\Roaming\\pypoetry\\venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "c:\\Users\\deanv\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\Roaming\\pypoetry\\venv\\Lib\\site-packages\\peft\\utils\\other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6832c592-227ef435434a187c41fae212;b760f982-690f-45d5-8288-d45552aea5b4)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B-Instruct.\n",
      "  warnings.warn(\n",
      "c:\\Users\\deanv\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\Roaming\\pypoetry\\venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "c:\\Users\\deanv\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\Roaming\\pypoetry\\venv\\Lib\\site-packages\\peft\\utils\\other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6832c598-3d3670a16072d22e396405db;29e5c7d4-3169-4628-8e05-dd97d07ba3cc)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B-Instruct.\n",
      "  warnings.warn(\n",
      "c:\\Users\\deanv\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\Roaming\\pypoetry\\venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "c:\\Users\\deanv\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\Roaming\\pypoetry\\venv\\Lib\\site-packages\\peft\\utils\\other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6832c59d-6c59e19f1888af1a43eb6635;28a5935b-7149-4141-84f8-dcc6084de597)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B-Instruct.\n",
      "  warnings.warn(\n",
      "c:\\Users\\deanv\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\Roaming\\pypoetry\\venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "c:\\Users\\deanv\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\Roaming\\pypoetry\\venv\\Lib\\site-packages\\peft\\utils\\other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6832c5a2-35f113ac1e637f36016521f9;f3542b20-8eca-4d28-a3ae-df7869acd179)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B-Instruct.\n",
      "  warnings.warn(\n",
      "c:\\Users\\deanv\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\Roaming\\pypoetry\\venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "c:\\Users\\deanv\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\Roaming\\pypoetry\\venv\\Lib\\site-packages\\peft\\utils\\other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6832c5a7-656a68f54e1938b62dd5a5a2;1d7fef41-56ec-4810-bc89-fba6aa3bd25f)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B-Instruct.\n",
      "  warnings.warn(\n",
      "c:\\Users\\deanv\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\Roaming\\pypoetry\\venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "c:\\Users\\deanv\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\Roaming\\pypoetry\\venv\\Lib\\site-packages\\peft\\utils\\other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6832c5ac-2a76e96148693d9b48c45afa;a6c7c66a-806a-472d-8981-b4e1684ce78b)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B-Instruct.\n",
      "  warnings.warn(\n",
      "c:\\Users\\deanv\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\Roaming\\pypoetry\\venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "c:\\Users\\deanv\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\Roaming\\pypoetry\\venv\\Lib\\site-packages\\peft\\utils\\other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6832c5b1-446323ef5ae3858a0b5e7dab;c8ed5943-7f6d-415e-9af4-8f650ca289c8)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B-Instruct.\n",
      "  warnings.warn(\n",
      "c:\\Users\\deanv\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\Roaming\\pypoetry\\venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "c:\\Users\\deanv\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\Roaming\\pypoetry\\venv\\Lib\\site-packages\\peft\\utils\\other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6832c5b6-5bc536435a5fa12f40735b4c;29e7722c-d7a8-4d30-a2f2-db00037f9dd1)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B-Instruct.\n",
      "  warnings.warn(\n",
      "c:\\Users\\deanv\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\Roaming\\pypoetry\\venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "c:\\Users\\deanv\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\Roaming\\pypoetry\\venv\\Lib\\site-packages\\peft\\utils\\other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6832c5bc-486bd5d61438974128ebd520;87463603-9f3e-459e-8e7e-c51757530d57)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B-Instruct.\n",
      "  warnings.warn(\n",
      "c:\\Users\\deanv\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\Roaming\\pypoetry\\venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Training completed!\n",
      "Final training loss: 1.7984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\deanv\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\Roaming\\pypoetry\\venv\\Lib\\site-packages\\peft\\utils\\other.py:1110: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6832c5bc-1bc5c7c24ff89e575ca49dc1;0af8cf26-5354-4019-bb08-1a413aa5856d)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B-Instruct.\n",
      "  warnings.warn(\n",
      "c:\\Users\\deanv\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\Roaming\\pypoetry\\venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:236: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Model saved to ./dnd_lora_final\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "training_output = trainer.train()\n",
    "\n",
    "print(f\"\\n✅ Training completed!\")\n",
    "print(f\"Final training loss: {training_output.training_loss:.4f}\")\n",
    "\n",
    "# Save the final model\n",
    "trainer.save_model(\"./dnd_lora_final\")\n",
    "tokenizer.save_pretrained(\"./dnd_lora_final\")\n",
    "\n",
    "print(\"💾 Model saved to ./dnd_lora_final\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144188ea",
   "metadata": {},
   "source": [
    "## Step 8: Test the Trained Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c69d2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(model, tokenizer, input_text, max_length=2048):\n",
    "    \"\"\"Generate a summary using the trained model.\"\"\"\n",
    "    # Create the prompt (without the target summary)\n",
    "    prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are an expert at summarizing Dungeons & Dragons sessions. Create engaging, detailed summaries that capture the story progression, character moments, combat encounters, and future plot hooks.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Summarize this D&D session transcript in 300-500 words:\n",
    "\n",
    "{input_text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=3584)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode the generated part only\n",
    "    generated_text = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    return generated_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "265104a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧪 Testing the trained model...\n",
      "\n",
      "Test example: chunk_12\n",
      "Input length: 25225 chars\n",
      "\n",
      "📝 Generated Summary:\n",
      "\\n\\n=== File 1: Critical Role plays Daggerheart ｜ Live One-Shot ｜ Open Beta_chunk_44_300_seconds (t=119.6s) ===\\n[120.1s - 122.0s] Is it you?\\n[122.5s - 123.0s] I think I am.\\n[123.2s - 123.4s] I think I am.\\n[123.8s - 124.1s] You're going to be okay.\\n[124.6s - 125.4s] I'm going to be okay.\\n[125.6s - 126.3s] So you're not going to be okay.\\n[126.6s - 127.4s] You're going to be okay.\\n[127.7s - 128.1s] I'm going to be okay.\\n[128.5s - 129.6s] You're going to be okay.\\n[130.4s - 131.1s] You're going to be okay.\\n[131.3s - 133.5s] So you're not going to be okay.\\n[133.9s - 135.2s] You're not going to be okay.\\n[135.3s - 136.2s] You're not going to be okay.\\n[136.3s - 136.5s] You're going to be okay.\\n[136.6s - 137.4s] You're going to be okay.\\n[137.6s - 139.3s] You're going to be okay.\\n[139.7s - 140.8s] So you're not going to be okay.\\n[141.0s - 141.3s] So you're not going to be okay.\\n[141.5s - 141.7s] You're going to be okay.\\n[141.8s - 142.1s] You're going to be okay.\\n[142.2s - 142.4s] You're going to be okay.\\n[142.5s - 142.8s] You're going to be okay.\\n[142.9s - 143.4s] You're going to be okay.\\n[143.6s - 144.0s] You're going to be okay.\\n[144.2s - 144.3s] You're going to be okay.\\n[144.4s - 144.6s] You're going to be okay.\\n[144.8s - 145.1s] So you're not going to be okay.\\n[145.4s - 145.6s] You're going to be okay.\\n[145.8s - 146.2s] So you're going to be okay.\\n[146.4s - 146.6s] You're going to be okay.\\n[146.7s - 147.2s] You're going to be okay.\\n[147.4s - 147.8s] You're going to be okay.\\n[148.0s - 148.2s] You're going to be okay.\\n[148.3s - 148.6s] You're going to be okay.\\n[148.8s - 149.2s] You're going to be okay.\\n[149.4s - 150.0s] So you're going to be okay.\\n[150.2s - 150.5s] You're going to be okay.\\n[150.6s - 151.1s] So you're going to be okay.\\n[151.3s - 153.5s] You're going to be okay.\\n[153.7s - 154.0s] You're going to be okay.\\n[154.2s - 155.2s] You're going to be okay.\\n[155.4s - 156.0s] You're going to be okay.\\n[156.2s - 156.8s] You're going to be okay.\\n[157.1s - 157.5s] You're going to be okay.\\n[157.7s - 158.4s] So you're going to be okay.\\n[158.6s - 159.5s] You're going to be okay.\\n[159.7s - 162.5s] You're going to be okay.\\n[162.7s - 162.8s] You're going to be okay.\\n[163.1s - 163.7s] You're going to be okay.\\n[164.0s - 164.1s] You're going to be okay.\\n[164.2s - 164.3s] You're going to be okay.\\n[164.5s - 164.8s] You're going to be okay.\\n[165.0s - 165.6s] You're going to be okay.\\n[165.7s - 166.5s] You're going to be okay.\\n[166.7s - 167.1s] You're going to be okay.\\n[167.2s - 167.6s] You're going to be okay.\\n[167.8s - 168.3s] You're going to be okay.\\n[168.4s - 168.6s] You're going to be okay.\\n[169.0s - 169.1s] You're going to be okay.\\n[169.2s - 169.3s] You're going to be okay.\\n[169.5s - 170.0s] You're going to be okay.\\n[170.2s - 170.4s] You're going to be okay.\\n[170.5s - 170.7s] You're going to be okay.\\n[170.9s - 171.3s] You're going to be okay.\\n[171.4s - 171.7s] You're going to be okay.\\n[171.8s - 172.0s] You're going to be okay.\\n[172.1s - 172.6s] You're going to be okay.\\n[172.8s - 173.0s] You're going to be okay.\\n[173.2s - 173.5s] You're going to be okay.\\n[173.6s - 174.0s] You're going to be okay.\\n[174.1s - 174.5s] You're going to be okay.\\n[174.7s - 175.1s] You're going to be okay.\\n[175.2s - 175.6s] You're going to be okay.\\n[175.7s - 176.3s] You're going to be okay.\\n[176.5s - 176.6s] You're going to be okay.\\n[176.7s - 176.9s] You're going to be okay.\\n[177.0s - 177.5s] You're going to be okay.\\n[177.6s - 177.8s] You're going to be okay.\\n[177.9s - 178.2s] You're going to be okay.\\n[178.3s - 178.4s] You're going to be okay.\\n[178.5s - 179.0s] You're going to be okay.\\n[179.1s - 179.4s] You're going to be okay.\\n[179.6s - 179.8s] You're going to be okay.\\n[179.9s - 180.0s] You're going to be okay.\\n[180.2s - 180.5s] You're going to be okay.\\n[180.6s - 181.2s] You're going to be okay.\\n[181.3s - 181.5s] You're going to be okay.\\n[181.6s - 181.7s] You're going to be okay.\\n[181.8s - 181.9s] You're going to be okay.\\n[182.0s - 183.3s] You're going to be okay.\\n[183.4s - 183.8s] You're going to be okay.\\n[183.9s - 184.0s] You're going to be okay.\\n[184.1s - 184.2s] You're going to be okay.\\n[184.3s - 184.4s] You're going to be okay.\\n[184.5s - 184.6s] You're going to be okay.\\n[184.7s - 184.9s] You're going to be okay.\\n[185.0s - 185.1s] You're going to be okay.\\n[185.2s - 185.4s] You're going to be okay.\\n[185.6s - 185.8s] You're going to be okay.\\n[185.9s - 186.0s] You're going to be okay.\\n[186.2s - 186.6s] You're going to be okay.\\n[186.7s - 186.8s] You're going to be okay.\\n[186.9s - 187.0s] You're going to be okay.\\n[187.1s - 187.2s] You're going to be okay.\\n[187.3s - 187.5s] You're going to be okay.\\n[187.6s - 187.7s] You're going to be okay.\\n[187.8s - 187.9s] You're going to be okay.\\n[188.0s - 188.1s] You're going to be okay.\\n[188.2s - 188.2s] You're going to be okay.\\n[188.3s - 188.4s] You're going to be okay.\\n[188.5s - 188.6s] You're going to be okay.\\n[188.7s - 189.0s] You're going to be okay.\\n[189.1s - 189.4s] You're going to be okay.\\n[189.6s - 189.7s] You're going to be okay.\\n[189.8s - 189.9s] You're going to be okay.\\n[190.0s - 190.1s] You're going to be okay.\\n[190.2s - 190.3s] You're going to be okay.\\n[190.4s - 190.5s] You're going to be okay.\\n[190.6s - 190.8s] You're going to be okay.\\n[190.9s - 191.0s] You're going to be okay.\\n[191.1s - 191.2s] You're going to be okay.\\n[191.3s - 191.4s] You're going to be okay.\\n[191.5s - 191.6s] You're going to be okay.\\n[191.7s - 191.8s] You're going to be okay.\\n[191.9s - 192.0s] You're going to be okay.\\n[192.1s - 192.2s] You're going to be okay.\\n[192.3s - 192.4s] You're going to be okay.\\n[192.5s - 192.6s] You're going to be okay.\\n[192.7s - 193.2s] You're going to be okay.\\n[193.3s - 193.4s] You're going to be okay.\\n[193.5s - 193.6s] You're going to be okay.\\n[193.7s - 193.8s] You're going to be okay.\\n[193.9s - 194.0s] You're going to be okay.\\n[194.1s - 194.3s] You're going to be okay.\\n[194.4s - 194.5s] You're going to be okay.\\n[194.6s - 194.7s] You're going to be okay.\\n[194.8s - 194.9s] You're going to be okay.\\n[195.0s - 195.0s] You're going to be okay.\\n[195.1s - 195.2s] You're going to be okay.\\n[195.3s - 195.4s] You're going to be okay.\\n[195.5s - 195.6s] You're going to be okay.\\n[195.7s - 195.8s] You're going to be okay.\\n[195.9s - 196.0s] You're going to be okay.\\n[196.1s - 196.2s] You're going to be okay.\\n[196.3s - 196.4s] You're going to be okay.\\n[196.5s - 196.6s] You're going to be okay.\\n[196.7s - 196.8s] You're going to be okay.\\n[196.9s - 197.0s] You're going to be okay.\\n[197.1s - 197.2s] You're going to be okay.\\n[197.3s - 197.5s] You're going to be okay.\\n[197.6s - 197.7s] You're going to be okay.\\n[197.8s - 197.9s] You're going to be okay.\\n[198.0s - 198.1s] You're going to be okay.\\n[198.2s - 198.4s] You're going to be okay.\\n[198.5s - 198.6s] You're going to be okay.\\n[198.7s - 198.8s] You're going to be okay.\\n[198.9s - 199.2s] You're going to be okay.\\n[199.3s - 199.5s] You're going to be okay.\\n[199.6s - 199.6s] You're going to be okay.\\n[199.7s - 200.0s] You're going to be okay.\\n[200.2s - 200.3s] You're going to be okay.\\n[200.4s - 200.6s] You're going to be okay.\\n[200.7s - 201.1s] You're going to be okay.\\n[201.2s - 201.3s] You're going to be okay.\\n[201.4s - 201.5s] You're going to be okay.\\n[201.6s - 201.7s] You're going to be okay.\\n[201.8s - 201.9s] You're going to be okay.\\n[202.0s - 203.0s] You're going to be okay.\\n[203.1s - 204.2s] You're going to be okay.\\n[204.4s - 205.0s] You're going to be okay.\\n[205.1s - 205.2s] You're going to be okay.\\n[205.3s - 205.4s] You're going to be okay.\\n[205.5s - 206.2s] You're going to be okay.\\n[206.3s - 206.6s] You're going to be okay.\\n[206.7s - 206.8s] You're going to be okay.\\n[206.9s - 207.0s] You're going to be okay.\\n[207.1s - 207.2s] You're going to be okay.\\n[207.3s - 207.4s] You're going to be okay.\\n[207.5s - 207.6s] You're going to be okay.\\n[207.7s - 207.9s] You're going to be okay.\\n[207.10s - 207.1s] You're going to be okay.\\n[207.2s - 207.3s] You're going to be okay.\\n[207.4s - 207.5s] You're going to be okay.\\n[207.6s - 207.7s] You're going to be okay.\\n[207.8s - 207.9s] You're going to be okay.\\n[207.10s - 208.0s] You're going to be okay.\\n[208.1s - 208.2s] You're going to be okay.\\n[208.3s - 208.4s] You're going to be okay.\\n[208.5s - 208.6s] You're going to be okay.\\n[208.7s - 208.8s] You're going to be okay.\\n[208.9s - 209.0s] You're going to be okay.\\n[209.1s - 209.2s] You're going to be okay.\\n[209.3s - 209.4s] You're going to be okay.\\n[209.5s - 209.6s] You're going to be okay.\\n[209.7s - 210.0s] You're going to be okay.\\n[210.1s - 210.2s] You're going to be okay.\\n[210.3s - 210.4s] You're going to be okay.\\n[210.5s - 210.6s] You're going to be okay.\\n[210.7s - 210.8s] You're going to be okay.\\n[210.9s - 211.0s] You're going to be okay.\\n[211.1s - 211.2s] You're going to be okay.\\n[211.3s - 211.4s] You're going to be okay.\\n[211.5s - 211.6\n",
      "\n",
      "📚 Reference Summary:\n",
      "In this thrilling episode of Critical Role's \"Daggerheart,\" the adventurers find themselves at the ominous source of a corrupted spring, the heart of a slime-infested river. As they traverse the treacherous terrain, they encounter a pulsing abscess surrounded by darkened vines, hinting at deeper dangers lurking beneath the surface. This scene sets the stage for an intense exploration and confrontation, highlighting the party's determination to cleanse the land of its corruption.\n",
      "\n",
      "Each character contributes uniquely to the unfolding adventure. As the party debates their next steps, one member bravely unfurls their wings to scout the area from above, discovering the corrupted spring at the hill's summit. Another character, grappling with past trauma triggered by the scene, bravely pushes forward, adding depth to their backstory and emotional resonance to the mission. Meanwhile, Zarlo steps forward with a medallion, invoking a divine presence in an attempt to reclaim the corrupted land, showing the character's spiritual connection and bravery.\n",
      "\n",
      "The episode kicks into high gear with the emergence of a colossal stone guardian, corrupted by the same malevolent force infecting the spring. This formidable foe, adorned with glowing purple eyes and streaks of the vile liquid, presents a daunting challenge for the party. The combat intensifies as vines erupt from the ground, ensnaring one of the adventurers and dealing damage, showcasing the party's resilience and quick thinking under pressure.\n",
      "\n",
      "Creative problem-solving shines through as the group employs both brute force and clever magic to tackle their obstacles. Shadow manipulation magic is used to bind the massive guardian, halting its advance and displaying the party's strategic prowess. A daring maneuver involving a grappling hook adds to the action, as two characters work together to topple the stone behemoth, resulting in a major wound to the enemy.\n",
      "\n",
      "The encounter concludes with the party's determined healer, Zarlo, clinging to the fallen guardian, attempting to cleanse its corruption using healing magic. This moment underscores the themes of redemption and the struggle against darkness, setting the stage for future confrontations with the malevolent forces at play.\n",
      "\n",
      "As the episode wraps up, the party's success in subduing the guardian and their ongoing efforts to purify the land suggest promising leads for future adventures. The mysterious nature of the corruption, combined with the personal stakes for each character, offers rich narrative threads to explore in subsequent sessions. This installment of \"Daggerheart\" leaves viewers eager to see how the heroes will continue their fight against the encroaching darkness and what revelations await them in their quest to restore balance to the land....\n",
      "\n",
      "📊 ROUGE Scores:\n",
      "  ROUGE1: 0.019\n",
      "  ROUGE2: 0.001\n",
      "  ROUGEL: 0.016\n"
     ]
    }
   ],
   "source": [
    "# Test on a validation example\n",
    "if len(eval_dataset) > 0:\n",
    "    print(\"\\n🧪 Testing the trained model...\")\n",
    "    \n",
    "    # Get a test example\n",
    "    test_idx = 0\n",
    "    test_example = training_data[train_size + test_idx]  # Use validation example\n",
    "    \n",
    "    print(f\"\\nTest example: {test_example['chunk_id']}\")\n",
    "    print(f\"Input length: {len(test_example['input_text'])} chars\")\n",
    "    \n",
    "    # Generate summary\n",
    "    generated_summary = generate_summary(\n",
    "        model, tokenizer, \n",
    "        test_example['input_text'][:2000],  # Truncate for testing\n",
    "        max_length=4096\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n📝 Generated Summary:\")\n",
    "    print(generated_summary)\n",
    "    \n",
    "    print(f\"\\n📚 Reference Summary:\")\n",
    "    print(test_example['target_summary'][:4096] + \"...\")\n",
    "    \n",
    "    # Quick ROUGE evaluation\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(test_example['target_summary'], generated_summary)\n",
    "    \n",
    "    print(f\"\\n📊 ROUGE Scores:\")\n",
    "    for metric, score in scores.items():\n",
    "        print(f\"  {metric.upper()}: {score.fmeasure:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b94db7",
   "metadata": {},
   "source": [
    "# Compare with Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "906d6900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "🔍 BASE MODEL vs FINE-TUNED MODEL COMPARISON\n",
      "================================================================================\n",
      "📥 Loading base model...\n",
      "✅ Base model loaded on: cuda:0\n",
      "✅ Fine-tuned model already loaded\n",
      "\n",
      "🧪 Testing on 3 examples...\n",
      "\n",
      "============================================================\n",
      "TEST EXAMPLE 1: chunk_12\n",
      "Input length: 25225 chars\n",
      "============================================================\n",
      "\n",
      "🤖 BASE MODEL Summary (Generated in 6.04s):\n",
      "============================================================\n",
      "**The Quest Begins**\n",
      "\n",
      "In the mist-shrouded mountains, a lone adventurer named Daggerheart finds themselves drawn to a mysterious spring nestled in the heart of the treacherous land. With a sense of foreboding, they approach the water's edge, where a tangled mess of dark vines forms a formidable barrier.\n",
      "\n",
      "As they examine the spring, their eyes widen in confusion. A pulsating, viscious substance emanates from the center, seemingly alive and aglow. A feeling of unease settles in, but not quite fear...\n",
      "============================================================\n",
      "\n",
      "🤖 FINE-TUNED MODEL Summary (Generated in 9.71s):\n",
      "============================================================\n",
      "As you gaze into your eyes, you try to recall any memories of what might have occurred during the attack. Your mind is filled with fragmented images of the chaotic battle that followed the sudden appearance of the Zeb or whatever entity was behind it. You vaguely recall seeing flashes of twisted, corrupted trees as the creatures seemed to emerge from the very earth itself, their bodies twisting and contorting in ways that defy human anatomy.\n",
      "\n",
      "The pain still lingers, a burning sensation that cour...\n",
      "============================================================\n",
      "\n",
      "📚 REFERENCE SUMMARY:\n",
      "============================================================\n",
      "In this thrilling episode of Critical Role's \"Daggerheart,\" the adventurers find themselves at the ominous source of a corrupted spring, the heart of a slime-infested river. As they traverse the treacherous terrain, they encounter a pulsing abscess surrounded by darkened vines, hinting at deeper dangers lurking beneath the surface. This scene sets the stage for an intense exploration and confrontation, highlighting the party's determination to cleanse the land of its corruption.\n",
      "\n",
      "Each character ...\n",
      "============================================================\n",
      "\n",
      "📊 BASE MODEL ROUGE Scores:\n",
      "  ROUGE1: 0.373\n",
      "  ROUGE2: 0.062\n",
      "  ROUGEL: 0.188\n",
      "\n",
      "📊 FINE-TUNED MODEL ROUGE Scores:\n",
      "  ROUGE1: 0.306\n",
      "  ROUGE2: 0.047\n",
      "  ROUGEL: 0.154\n",
      "\n",
      "📈 IMPROVEMENT ANALYSIS:\n",
      "  ROUGE1: -0.066 📉\n",
      "  ROUGE2: -0.015 📉\n",
      "  ROUGEL: -0.033 📉\n",
      "\n",
      "⚡ GENERATION SPEED:\n",
      "  Base model: 6.04s\n",
      "  Fine-tuned: 9.71s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "TEST EXAMPLE 2: chunk_13\n",
      "Input length: 25963 chars\n",
      "============================================================\n",
      "\n",
      "🤖 BASE MODEL Summary (Generated in 6.12s):\n",
      "============================================================\n",
      "**Session Summary**\n",
      "\n",
      "The adventurers found themselves trapped by a massive vine creature, Boco, in a dark and twisted forest. The villainous force was corrupting the land, and the players had been tasked with freeing the innocent villager who was being held captive.\n",
      "\n",
      "As they began to fight, the player who cast \"Too Stress\" unleashed their divine power, causing a burst of energy that pushed back against the vine. However, instead of harming the players directly, it seemed to affect the corrupted ...\n",
      "============================================================\n",
      "\n",
      "🤖 FINE-TUNED MODEL Summary (Generated in 11.73s):\n",
      "============================================================\n",
      "As the sun sets over the ravaged city, you find yourselves standing atop a crumbling stone wall, the ruins of a once-great metropolis stretching out before you. The air is thick with the stench of death and decay, and the sound of distant screams echoes through the desolate landscape.\n",
      "\n",
      "You've been traveling for days, seeking refuge from the horrors that lurk in the shadows. As you look around, you notice that the city seems to be shifting and changing, as if it's being pulled apart by some unsee...\n",
      "============================================================\n",
      "\n",
      "📚 REFERENCE SUMMARY:\n",
      "============================================================\n",
      "In this action-packed session of Critical Role, the party finds themselves entangled in a tense battle against a corrupted entity known as Boco, amidst the shadowy vines and toxic environment. The episode is filled with strategic maneuvers, compelling character moments, and high-stakes rolls that keep the audience at the edge of their seats.\n",
      "\n",
      "The session kicks off with the party confronting the blackened corruption within Boco, using divine powers to burn away the dark tendrils. This sets the st...\n",
      "============================================================\n",
      "\n",
      "📊 BASE MODEL ROUGE Scores:\n",
      "  ROUGE1: 0.402\n",
      "  ROUGE2: 0.077\n",
      "  ROUGEL: 0.190\n",
      "\n",
      "📊 FINE-TUNED MODEL ROUGE Scores:\n",
      "  ROUGE1: 0.307\n",
      "  ROUGE2: 0.035\n",
      "  ROUGEL: 0.145\n",
      "\n",
      "📈 IMPROVEMENT ANALYSIS:\n",
      "  ROUGE1: -0.095 📉\n",
      "  ROUGE2: -0.042 📉\n",
      "  ROUGEL: -0.044 📉\n",
      "\n",
      "⚡ GENERATION SPEED:\n",
      "  Base model: 6.12s\n",
      "  Fine-tuned: 11.73s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "TEST EXAMPLE 3: chunk_14\n",
      "Input length: 12216 chars\n",
      "============================================================\n",
      "\n",
      "🤖 BASE MODEL Summary (Generated in 6.26s):\n",
      "============================================================\n",
      "**Dungeon Summary**\n",
      "\n",
      "The party finds themselves in the midst of a brutal battle against a massive, terrifying creature known as \"The Abscess\". They have managed to defeat the creature, but not without taking significant damage.\n",
      "\n",
      "As they approach the location where the creature is located, they notice that they're surrounded by a massive amount of glowing, pulsating green tissue. Suddenly, the creature emerges, its eyes glowing purple and then fading to white. The party leader, Daggerheart, swing...\n",
      "============================================================\n",
      "\n",
      "🤖 FINE-TUNED MODEL Summary (Generated in 11.61s):\n",
      "============================================================\n",
      "This file contains a critical role play from Open Beta chunk 52 of the live one-shot Daggerheart chapter, which is part of the open beta for Dungeons & Dragon 5th edition. Here's a comprehensive summary of the session within the 300-word limit:\n",
      "\n",
      "The session opens with Thalane casting a zone of toxic slime, damaging all enemies within its area of effect. Meanwhile, the party consists of Eira, a half-elf ranger, Arin, a human rogue, and Lysander, a dwarf cleric. They're currently fighting against ...\n",
      "============================================================\n",
      "\n",
      "📚 REFERENCE SUMMARY:\n",
      "============================================================\n",
      "In a thrilling session of Critical Role's Daggerheart one-shot, the party faced a significant challenge with a mysterious and dangerous growth in their environment. As the session unfolded, they discovered that the malignant abscess was spreading, threatening the very essence of the forest they were in. This episode was marked by intense combat, extraordinary teamwork, and a cascade of revelations that set the stage for future adventures.\n",
      "\n",
      "The party's primary combat encounter was against the for...\n",
      "============================================================\n",
      "\n",
      "📊 BASE MODEL ROUGE Scores:\n",
      "  ROUGE1: 0.378\n",
      "  ROUGE2: 0.075\n",
      "  ROUGEL: 0.176\n",
      "\n",
      "📊 FINE-TUNED MODEL ROUGE Scores:\n",
      "  ROUGE1: 0.346\n",
      "  ROUGE2: 0.056\n",
      "  ROUGEL: 0.162\n",
      "\n",
      "📈 IMPROVEMENT ANALYSIS:\n",
      "  ROUGE1: -0.032 📉\n",
      "  ROUGE2: -0.020 📉\n",
      "  ROUGEL: -0.014 📉\n",
      "\n",
      "⚡ GENERATION SPEED:\n",
      "  Base model: 6.26s\n",
      "  Fine-tuned: 11.61s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "📊 OVERALL PERFORMANCE COMPARISON\n",
      "================================================================================\n",
      "Metric       Base     Fine-tuned   Improvement  Status\n",
      "------------------------------------------------------------\n",
      "ROUGE1       0.384    0.320        -0.064      ❌ Worse\n",
      "ROUGE2       0.071    0.046        -0.025      ❌ Worse\n",
      "ROUGEL       0.184    0.154        -0.031      ❌ Worse\n",
      "\n",
      "🎯 CONCLUSION:\n",
      "❌ Fine-tuning didn't improve performance. Need more data or different approach.\n",
      "📈 Average improvement across all metrics: -0.040\n",
      "\n",
      "🧹 Cleaned up base model from memory\n"
     ]
    }
   ],
   "source": [
    "# Compare Base Model vs Fine-tuned Model Performance\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from rouge_score import rouge_scorer\n",
    "import time\n",
    "\n",
    "def load_base_model():\n",
    "    \"\"\"Load the original base model without LoRA.\"\"\"\n",
    "    print(\"📥 Loading base model...\")\n",
    "    \n",
    "    base_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=hf_token)\n",
    "    base_tokenizer.pad_token = base_tokenizer.eos_token\n",
    "    \n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        token=hf_token,\n",
    "        quantization_config=bnb_config,  # Same config as fine-tuned\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Base model loaded on: {base_model.device}\")\n",
    "    return base_model, base_tokenizer\n",
    "\n",
    "def generate_summary_comparison(model, tokenizer, input_text, model_name, max_length=400):\n",
    "    \"\"\"Generate summary with consistent parameters for fair comparison.\"\"\"\n",
    "    \n",
    "    # Truncate input to same length for both models\n",
    "    input_text = input_text[:3000]\n",
    "    \n",
    "    # Use the same prompt format\n",
    "    prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are an expert at summarizing Dungeons & Dragons sessions. Create engaging, detailed summaries that capture the story progression, character moments, combat encounters, and future plot hooks.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Summarize this D&D session transcript in 300-500 words:\n",
    "\n",
    "{input_text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1500)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate with identical settings\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            temperature=0.8,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            top_k=50,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            no_repeat_ngram_size=3,\n",
    "        )\n",
    "    \n",
    "    generation_time = time.time() - start_time\n",
    "    \n",
    "    # Decode\n",
    "    generated_text = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    generated_text = generated_text.strip()\n",
    "    \n",
    "    # Clean up\n",
    "    if \"assistant\" in generated_text:\n",
    "        generated_text = generated_text.split(\"assistant\")[-1].strip()\n",
    "    \n",
    "    print(f\"\\n🤖 {model_name} Summary (Generated in {generation_time:.2f}s):\")\n",
    "    print(\"=\" * 60)\n",
    "    print(generated_text[:500] + (\"...\" if len(generated_text) > 500 else \"\"))\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return generated_text, generation_time\n",
    "\n",
    "def calculate_rouge_scores(reference, generated, model_name):\n",
    "    \"\"\"Calculate and display ROUGE scores.\"\"\"\n",
    "    if not generated or len(generated.strip()) < 10:\n",
    "        print(f\"⚠️ {model_name}: Generated text too short for evaluation\")\n",
    "        return None\n",
    "    \n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference, generated)\n",
    "    \n",
    "    print(f\"\\n📊 {model_name} ROUGE Scores:\")\n",
    "    rouge_dict = {}\n",
    "    for metric, score in scores.items():\n",
    "        print(f\"  {metric.upper()}: {score.fmeasure:.3f}\")\n",
    "        rouge_dict[metric] = score.fmeasure\n",
    "    \n",
    "    return rouge_dict\n",
    "\n",
    "def compare_models():\n",
    "    \"\"\"Run comprehensive comparison between base and fine-tuned models.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"🔍 BASE MODEL vs FINE-TUNED MODEL COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load base model\n",
    "    base_model, base_tokenizer = load_base_model()\n",
    "    \n",
    "    # Use your existing fine-tuned model (already loaded as 'model')\n",
    "    print(\"✅ Fine-tuned model already loaded\")\n",
    "    \n",
    "    # Test on multiple examples if available\n",
    "    test_examples = []\n",
    "    if len(eval_dataset) > 0:\n",
    "        # Use validation examples\n",
    "        for i in range(min(3, len(eval_dataset))):  # Test up to 3 examples\n",
    "            test_examples.append(training_data[train_size + i])\n",
    "    else:\n",
    "        # Use some training examples\n",
    "        for i in range(min(3, len(training_data))):\n",
    "            test_examples.append(training_data[i])\n",
    "    \n",
    "    print(f\"\\n🧪 Testing on {len(test_examples)} examples...\")\n",
    "    \n",
    "    all_base_scores = []\n",
    "    all_finetuned_scores = []\n",
    "    \n",
    "    for idx, test_example in enumerate(test_examples):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"TEST EXAMPLE {idx + 1}: {test_example['chunk_id']}\")\n",
    "        print(f\"Input length: {len(test_example['input_text'])} chars\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Generate with base model\n",
    "        base_summary, base_time = generate_summary_comparison(\n",
    "            base_model, base_tokenizer, \n",
    "            test_example['input_text'], \n",
    "            \"BASE MODEL\"\n",
    "        )\n",
    "        \n",
    "        # Generate with fine-tuned model  \n",
    "        finetuned_summary, ft_time = generate_summary_comparison(\n",
    "            model, tokenizer,\n",
    "            test_example['input_text'],\n",
    "            \"FINE-TUNED MODEL\"\n",
    "        )\n",
    "        \n",
    "        # Show reference\n",
    "        print(f\"\\n📚 REFERENCE SUMMARY:\")\n",
    "        print(\"=\" * 60)\n",
    "        print(test_example['target_summary'][:500] + (\"...\" if len(test_example['target_summary']) > 500 else \"\"))\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Calculate ROUGE scores\n",
    "        base_scores = calculate_rouge_scores(test_example['target_summary'], base_summary, \"BASE MODEL\")\n",
    "        ft_scores = calculate_rouge_scores(test_example['target_summary'], finetuned_summary, \"FINE-TUNED MODEL\")\n",
    "        \n",
    "        if base_scores and ft_scores:\n",
    "            all_base_scores.append(base_scores)\n",
    "            all_finetuned_scores.append(ft_scores)\n",
    "            \n",
    "            # Show improvement\n",
    "            print(f\"\\n📈 IMPROVEMENT ANALYSIS:\")\n",
    "            for metric in ['rouge1', 'rouge2', 'rougeL']:\n",
    "                improvement = ft_scores[metric] - base_scores[metric]\n",
    "                direction = \"📈\" if improvement > 0 else \"📉\" if improvement < 0 else \"➡️\"\n",
    "                print(f\"  {metric.upper()}: {improvement:+.3f} {direction}\")\n",
    "        \n",
    "        # Performance comparison\n",
    "        print(f\"\\n⚡ GENERATION SPEED:\")\n",
    "        print(f\"  Base model: {base_time:.2f}s\")\n",
    "        print(f\"  Fine-tuned: {ft_time:.2f}s\")\n",
    "        \n",
    "        print(f\"\\n\" + \"-\"*80)\n",
    "    \n",
    "    # Overall comparison\n",
    "    if all_base_scores and all_finetuned_scores:\n",
    "        print(f\"\\n\" + \"=\"*80)\n",
    "        print(\"📊 OVERALL PERFORMANCE COMPARISON\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Calculate averages\n",
    "        metrics = ['rouge1', 'rouge2', 'rougeL']\n",
    "        \n",
    "        print(f\"{'Metric':<12} {'Base':<8} {'Fine-tuned':<12} {'Improvement':<12} {'Status'}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for metric in metrics:\n",
    "            base_avg = sum(scores[metric] for scores in all_base_scores) / len(all_base_scores)\n",
    "            ft_avg = sum(scores[metric] for scores in all_finetuned_scores) / len(all_finetuned_scores)\n",
    "            improvement = ft_avg - base_avg\n",
    "            \n",
    "            status = \"✅ Better\" if improvement > 0.01 else \"❌ Worse\" if improvement < -0.01 else \"➡️ Similar\"\n",
    "            \n",
    "            print(f\"{metric.upper():<12} {base_avg:<8.3f} {ft_avg:<12.3f} {improvement:+.3f}      {status}\")\n",
    "        \n",
    "        # Conclusion\n",
    "        avg_improvement = sum(\n",
    "            sum(ft_scores[m] for m in metrics) - sum(base_scores[m] for m in metrics)\n",
    "            for base_scores, ft_scores in zip(all_base_scores, all_finetuned_scores)\n",
    "        ) / (len(all_base_scores) * 3)\n",
    "        \n",
    "        print(f\"\\n🎯 CONCLUSION:\")\n",
    "        if avg_improvement > 0.02:\n",
    "            print(\"✅ Fine-tuning was successful! The model shows clear improvement.\")\n",
    "        elif avg_improvement > 0.005:\n",
    "            print(\"⚠️ Fine-tuning shows modest improvement. Consider more data or training.\")\n",
    "        else:\n",
    "            print(\"❌ Fine-tuning didn't improve performance. Need more data or different approach.\")\n",
    "        \n",
    "        print(f\"📈 Average improvement across all metrics: {avg_improvement:+.3f}\")\n",
    "    \n",
    "    # Clean up base model to free memory\n",
    "    del base_model, base_tokenizer\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"\\n🧹 Cleaned up base model from memory\")\n",
    "\n",
    "# Run the comparison\n",
    "compare_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cf770d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d642f1ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

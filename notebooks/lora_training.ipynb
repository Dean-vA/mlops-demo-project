{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfad92c3",
   "metadata": {},
   "source": [
    "# D&D Summarization LoRA Training - Simple Case\n",
    "# Training Llama 3.1 1B on a few transcript/summary pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c81cb07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer\n",
    "import dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019072f1",
   "metadata": {},
   "source": [
    "# Check GPU availability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbd76397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 5090\n",
      "VRAM: 34.2 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17fde4e",
   "metadata": {},
   "source": [
    "## Step 1: Load and Prepare Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2ff8f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data(summaries_dir=\"../data/summaries\", transcripts_dir=\"../data/combined_transcripts\"):\n",
    "    \"\"\"Load the 14 training pairs from your summary JSON files.\"\"\"\n",
    "    print(f\"Loading training data from {summaries_dir}\")\n",
    "    \n",
    "    summaries_path = Path(summaries_dir)\n",
    "    # print top 5 files in the directory\n",
    "    print(\"Files in summaries directory:\")\n",
    "    for file in sorted(summaries_path.glob(\"chunk_*_summary.json\"))[:5]:\n",
    "        print(f\" - {file.name}\")\n",
    "\n",
    "    training_pairs = []\n",
    "    \n",
    "    # Load all summary files\n",
    "    for json_file in sorted(summaries_path.glob(\"chunk_*_summary.json\")):\n",
    "        print(f\"Processing {json_file.name}\")\n",
    "        try:\n",
    "            with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Extract the combined transcript (this will be our input)\n",
    "            # You'll need to load the corresponding simple.json file for the transcript\n",
    "            chunk_num = data['chunk_number']\n",
    "            simple_file = summaries_path.parent / transcripts_dir / f\"chunk_{chunk_num:02d}_simple.json\"\n",
    "            \n",
    "            if simple_file.exists():\n",
    "                with open(simple_file, 'r', encoding='utf-8') as f:\n",
    "                    chunk_data = json.load(f)\n",
    "                \n",
    "                training_pair = {\n",
    "                    'chunk_id': f\"chunk_{chunk_num:02d}\",\n",
    "                    'input_text': chunk_data['combined_transcript'],\n",
    "                    'target_summary': data['summary'],\n",
    "                    'duration_minutes': data['duration_minutes'],\n",
    "                    'word_count': data['word_count']\n",
    "                }\n",
    "                training_pairs.append(training_pair)            \n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {json_file}: {e}\")\n",
    "\n",
    "    print(f\"Loaded {len(training_pairs)} training pairs\")\n",
    "    return training_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22587693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data from ../data/summaries\n",
      "Files in summaries directory:\n",
      " - chunk_01_summary.json\n",
      " - chunk_02_summary.json\n",
      " - chunk_03_summary.json\n",
      " - chunk_04_summary.json\n",
      " - chunk_05_summary.json\n",
      "Processing chunk_01_summary.json\n",
      "Processing chunk_02_summary.json\n",
      "Processing chunk_03_summary.json\n",
      "Processing chunk_04_summary.json\n",
      "Processing chunk_05_summary.json\n",
      "Processing chunk_06_summary.json\n",
      "Processing chunk_07_summary.json\n",
      "Processing chunk_08_summary.json\n",
      "Processing chunk_09_summary.json\n",
      "Processing chunk_10_summary.json\n",
      "Processing chunk_11_summary.json\n",
      "Processing chunk_12_summary.json\n",
      "Processing chunk_13_summary.json\n",
      "Processing chunk_14_summary.json\n",
      "Loaded 14 training pairs\n"
     ]
    }
   ],
   "source": [
    "# Load your data\n",
    "training_data = load_training_data(transcripts_dir=\"../data/combined_transcripts_20min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2108e30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Example Training Pair:\n",
      "Chunk: chunk_01\n",
      "Duration: 20.0 minutes\n",
      "Input length: 1191 chars\n",
      "Summary length: 3161 chars\n",
      "\n",
      "Input preview: \\n\\n=== File 1: Critical Role plays Daggerheart ÔΩú Live One-Shot ÔΩú Open Beta_chunk_0_300_seconds (t=0.0s) ===\\n[0.0s - 300.0s] \\n\\n\\n=== File 2: Critical Role plays Daggerheart ÔΩú Live One-Shot ÔΩú Open B...\n",
      "\n",
      "Target summary preview: In this thrilling 20-minute session of Critical Role's Daggerheart one-shot, the party embarked on an exhilarating adventure filled with mystery and magic. As they celebrated their nine-year anniversa...\n"
     ]
    }
   ],
   "source": [
    "# Display first example\n",
    "\n",
    "example = training_data[0]\n",
    "print(f\"\\nüìã Example Training Pair:\")\n",
    "print(f\"Chunk: {example['chunk_id']}\")\n",
    "print(f\"Duration: {example['duration_minutes']:.1f} minutes\")\n",
    "print(f\"Input length: {len(example['input_text'])} chars\")\n",
    "print(f\"Summary length: {len(example['target_summary'])} chars\")\n",
    "print(f\"\\nInput preview: {example['input_text'][:200]}...\")\n",
    "print(f\"\\nTarget summary preview: {example['target_summary'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a5381f",
   "metadata": {},
   "source": [
    "## Step 2: Setup Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91a47371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Loading model: microsoft/DialoGPT-medium\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\deanv\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\Roaming\\pypoetry\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\deanv\\.cache\\huggingface\\hub\\models--microsoft--DialoGPT-medium. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "# MODEL_NAME = \"meta-llama/Llama-3.1-1B\"  # or \"meta-llama/Llama-3.1-1B-Instruct\"\n",
    "MODEL_NAME = \"microsoft/DialoGPT-medium\" \n",
    "hf_token = os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "\n",
    "print(f\"\\nü§ñ Loading model: {MODEL_NAME}\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    token=hf_token \n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6a2470e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# # Load model in 4-bit for memory efficiency (optional, remove if you have enough VRAM)\n",
    "# from transformers import BitsAndBytesConfig\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,  # Remove this line if you want full precision\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(f\"Model loaded on: {model.device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bcc3ee",
   "metadata": {},
   "source": [
    "## Step 3: Configure LoRA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9aa3fd80",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Target modules {'gate_proj', 'down_proj', 'up_proj', 'k_proj', 'o_proj', 'q_proj', 'v_proj'} not found in the base model. Please check the target modules and try again.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m      1\u001b[39m lora_config = LoraConfig(\n\u001b[32m      2\u001b[39m     task_type=TaskType.CAUSAL_LM,\n\u001b[32m      3\u001b[39m     r=\u001b[32m16\u001b[39m,                    \u001b[38;5;66;03m# Rank - start small for 14 samples\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m     use_rslora=\u001b[38;5;28;01mFalse\u001b[39;00m,       \u001b[38;5;66;03m# Set to True for better stability with larger ranks\u001b[39;00m\n\u001b[32m     17\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m model = \u001b[43mget_peft_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m model.print_trainable_parameters()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\deanv\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\Roaming\\pypoetry\\venv\\Lib\\site-packages\\peft\\mapping_func.py:123\u001b[39m, in \u001b[36mget_peft_model\u001b[39m\u001b[34m(model, peft_config, adapter_name, mixed, autocast_adapter_dtype, revision, low_cpu_mem_usage)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m peft_config.is_prompt_learning:\n\u001b[32m    122\u001b[39m     peft_config = _prepare_prompt_learning_config(peft_config, model_config)\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtask_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\deanv\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\Roaming\\pypoetry\\venv\\Lib\\site-packages\\peft\\peft_model.py:1722\u001b[39m, in \u001b[36mPeftModelForCausalLM.__init__\u001b[39m\u001b[34m(self, model, peft_config, adapter_name, **kwargs)\u001b[39m\n\u001b[32m   1719\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m   1720\u001b[39m     \u001b[38;5;28mself\u001b[39m, model: torch.nn.Module, peft_config: PeftConfig, adapter_name: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m\"\u001b[39m, **kwargs\n\u001b[32m   1721\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1722\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1723\u001b[39m     \u001b[38;5;28mself\u001b[39m.base_model_prepare_inputs_for_generation = \u001b[38;5;28mself\u001b[39m.base_model.prepare_inputs_for_generation\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\deanv\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\Roaming\\pypoetry\\venv\\Lib\\site-packages\\peft\\peft_model.py:132\u001b[39m, in \u001b[36mPeftModel.__init__\u001b[39m\u001b[34m(self, model, peft_config, adapter_name, autocast_adapter_dtype, low_cpu_mem_usage)\u001b[39m\n\u001b[32m    130\u001b[39m     ctx = init_empty_weights \u001b[38;5;28;01mif\u001b[39;00m low_cpu_mem_usage \u001b[38;5;28;01melse\u001b[39;00m nullcontext\n\u001b[32m    131\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx():\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m         \u001b[38;5;28mself\u001b[39m.base_model = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[38;5;28mself\u001b[39m.set_additional_trainable_modules(peft_config, adapter_name)\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.base_model, \u001b[33m\"\u001b[39m\u001b[33m_cast_adapter_dtype\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\deanv\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\Roaming\\pypoetry\\venv\\Lib\\site-packages\\peft\\tuners\\lora\\model.py:142\u001b[39m, in \u001b[36mLoraModel.__init__\u001b[39m\u001b[34m(self, model, config, adapter_name, low_cpu_mem_usage)\u001b[39m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, config, adapter_name, low_cpu_mem_usage: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\deanv\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\Roaming\\pypoetry\\venv\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:180\u001b[39m, in \u001b[36mBaseTuner.__init__\u001b[39m\u001b[34m(self, model, peft_config, adapter_name, low_cpu_mem_usage)\u001b[39m\n\u001b[32m    178\u001b[39m \u001b[38;5;28mself\u001b[39m._pre_injection_hook(\u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.peft_config[adapter_name], adapter_name)\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m peft_config != PeftType.XLORA \u001b[38;5;129;01mor\u001b[39;00m peft_config[adapter_name] != PeftType.XLORA:\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minject_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[38;5;66;03m# Copy the peft_config in the injected model.\u001b[39;00m\n\u001b[32m    183\u001b[39m \u001b[38;5;28mself\u001b[39m.model.peft_config = \u001b[38;5;28mself\u001b[39m.peft_config\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\deanv\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\Roaming\\pypoetry\\venv\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:527\u001b[39m, in \u001b[36mBaseTuner.inject_adapter\u001b[39m\u001b[34m(self, model, adapter_name, autocast_adapter_dtype, low_cpu_mem_usage)\u001b[39m\n\u001b[32m    525\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(peft_config, \u001b[33m\"\u001b[39m\u001b[33mlayers_pattern\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    526\u001b[39m         error_msg += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m You also specified \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlayers_pattern\u001b[39m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpeft_config.layers_pattern\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m527\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg)\n\u001b[32m    528\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    529\u001b[39m     \u001b[38;5;66;03m# Some modules did not match and some matched but were excluded\u001b[39;00m\n\u001b[32m    530\u001b[39m     error_msg = (\n\u001b[32m    531\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo modules were targeted for adaptation. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    532\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThis might be caused by a combination of mismatched target modules and excluded modules. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    533\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease check your `target_modules` and `exclude_modules` configuration.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    534\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Target modules {'gate_proj', 'down_proj', 'up_proj', 'k_proj', 'o_proj', 'q_proj', 'v_proj'} not found in the base model. Please check the target modules and try again."
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=16,                    # Rank - start small for 14 samples\n",
    "    lora_alpha=32,          # Scaling parameter\n",
    "    lora_dropout=0.1,       # Dropout for regularization\n",
    "    target_modules=[        # Target attention modules\n",
    "        \"q_proj\",\n",
    "        \"k_proj\", \n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",        # Also target MLP for better performance\n",
    "        \"up_proj\",\n",
    "        \"down_proj\"\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    use_rslora=False,       # Set to True for better stability with larger ranks\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581e7df9",
   "metadata": {},
   "source": [
    "## Step 4: Create Training Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aea2e655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_prompt(input_text, target_summary):\n",
    "    \"\"\"Create a formatted prompt for training.\"\"\"\n",
    "    prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are an expert at summarizing Dungeons & Dragons sessions. Create engaging, detailed summaries that capture the story progression, character moments, combat encounters, and future plot hooks.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Summarize this D&D session transcript in 300-500 words:\n",
    "\n",
    "{input_text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{target_summary}<|eot_id|>\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78457093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize the training examples.\"\"\"\n",
    "    # Create full prompts\n",
    "    prompts = [create_training_prompt(inp, target) \n",
    "               for inp, target in zip(examples['input_text'], examples['target_summary'])]\n",
    "    \n",
    "    # Tokenize with truncation for long sequences\n",
    "    tokenized = tokenizer(\n",
    "        prompts,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        max_length=4096,  # Adjust based on your GPU memory\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    # For causal LM, labels are the same as input_ids\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    \n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c58c525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to HuggingFace Dataset\n",
    "df = pd.DataFrame(training_data)\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Tokenize dataset\n",
    "print(\"\\nüî§ Tokenizing dataset...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "\n",
    "print(f\"Dataset size: {len(tokenized_dataset)}\")\n",
    "print(f\"Average sequence length: {np.mean([len(x) for x in tokenized_dataset['input_ids']]):.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed150bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/validation (with only 14 samples, we'll use a simple split)\n",
    "train_size = int(0.8 * len(tokenized_dataset))  # 11 for training, 3 for validation\n",
    "train_dataset = tokenized_dataset.select(range(train_size))\n",
    "eval_dataset = tokenized_dataset.select(range(train_size, len(tokenized_dataset)))\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9a212c",
   "metadata": {},
   "source": [
    "## Step 5: Training Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36614fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./dnd_lora_checkpoints\",\n",
    "    num_train_epochs=5,              # More epochs for small dataset\n",
    "    per_device_train_batch_size=1,   # Small batch due to long sequences\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=8,   # Effective batch size = 8\n",
    "    learning_rate=2e-4,              # Standard LoRA learning rate\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=1,                 # Log every step for small dataset\n",
    "    evaluation_strategy=\"epoch\",     # Evaluate after each epoch\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    warmup_steps=2,                  # Small warmup for small dataset\n",
    "    fp16=False,                      # Use bf16 instead if supported\n",
    "    bf16=True,\n",
    "    dataloader_num_workers=0,        # Avoid multiprocessing issues\n",
    "    remove_unused_columns=False,\n",
    "    report_to=None,                  # Disable wandb/tensorboard for now\n",
    ")\n",
    "\n",
    "# Data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # We're doing causal LM, not masked LM\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33003970",
   "metadata": {},
   "source": [
    "## Step 6: Initialize Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ebc075",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"\\nüöÄ Starting training...\")\n",
    "print(f\"Training {len(train_dataset)} samples for {training_args.num_train_epochs} epochs\")\n",
    "print(f\"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28b3dc1",
   "metadata": {},
   "source": [
    "## Step 7: Train the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd309ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "training_output = trainer.train()\n",
    "\n",
    "print(f\"\\n‚úÖ Training completed!\")\n",
    "print(f\"Final training loss: {training_output.training_loss:.4f}\")\n",
    "\n",
    "# Save the final model\n",
    "trainer.save_model(\"./dnd_lora_final\")\n",
    "tokenizer.save_pretrained(\"./dnd_lora_final\")\n",
    "\n",
    "print(\"üíæ Model saved to ./dnd_lora_final\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144188ea",
   "metadata": {},
   "source": [
    "## Step 8: Test the Trained Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c69d2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(model, tokenizer, input_text, max_length=512):\n",
    "    \"\"\"Generate a summary using the trained model.\"\"\"\n",
    "    # Create the prompt (without the target summary)\n",
    "    prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are an expert at summarizing Dungeons & Dragons sessions. Create engaging, detailed summaries that capture the story progression, character moments, combat encounters, and future plot hooks.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Summarize this D&D session transcript in 300-500 words:\n",
    "\n",
    "{input_text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=3584)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode the generated part only\n",
    "    generated_text = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    return generated_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265104a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a validation example\n",
    "if len(eval_dataset) > 0:\n",
    "    print(\"\\nüß™ Testing the trained model...\")\n",
    "    \n",
    "    # Get a test example\n",
    "    test_idx = 0\n",
    "    test_example = training_data[train_size + test_idx]  # Use validation example\n",
    "    \n",
    "    print(f\"\\nTest example: {test_example['chunk_id']}\")\n",
    "    print(f\"Input length: {len(test_example['input_text'])} chars\")\n",
    "    \n",
    "    # Generate summary\n",
    "    generated_summary = generate_summary(\n",
    "        model, tokenizer, \n",
    "        test_example['input_text'][:2000],  # Truncate for testing\n",
    "        max_length=300\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìù Generated Summary:\")\n",
    "    print(generated_summary)\n",
    "    \n",
    "    print(f\"\\nüìö Reference Summary:\")\n",
    "    print(test_example['target_summary'][:300] + \"...\")\n",
    "    \n",
    "    # Quick ROUGE evaluation\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(test_example['target_summary'], generated_summary)\n",
    "    \n",
    "    print(f\"\\nüìä ROUGE Scores:\")\n",
    "    for metric, score in scores.items():\n",
    "        print(f\"  {metric.upper()}: {score.fmeasure:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
